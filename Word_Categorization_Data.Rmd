---
title: "Word Categorization Data"
author: "Luisa Balzus"
date: "18 Juli 2019"
output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  tidy = TRUE,
  message = FALSE,
  warning = FALSE,
  dpi = 150,
  comment = NA
)
```


```{r libraries, include = FALSE}
library(pastecs)                 # for descriptive stats
library(ggplot2)                 # for plots
library(knitr)                   # for nice tables in html
library(kableExtra)              # for nice tables in html
library(ez)                      # for my ANOVAs
library(papaja)                  # for visualizing Anova with papaja
library(afex)                    # for Anova that can be visualized in APA style with papaja and for ANOVA plot
library(lattice)                 # for single subject qq plot (qqmath)
library(e1071)                   # for functions skewness and kurtosis
library(grid)                    # for saving grobarranged plots
library(gridExtra)               # for arranging plots in a grid
library(tidyr)                   # to do plot for all columns of a df
library(emmeans)                 # for ANOVA follow-up tests
library(ggsignif)                # for adding significance bars in plots
library(plyr)                    # for ddply()
library(dplyr)                   # for mutating
library(lme4)                    # for (G)LMMs
library(lmerTest)                # for p values for tests for fixed effects (Satterthwaite's method for approximating degrees of freedom for the t and F tests)
library(MASS)                    # for box cox and contrast definition
library("GeneralizedHyperbolic") # for fitting inverse gaussian distribution
library(car)                     # for qqp() - QQplot
library(sjPlot)                  # for tab_model
library(Rmisc)                   # for summarySEwithin

# clear environment
rm(list=ls())

# force R to not use exponential notation
options(scipen = 999)

# reading in datafile (only single trial data, not df4save)
datafiles <- list.files("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses", pattern = "Data_Single_Trial") # replace pattern with .rda, if I also want to read in aggregated rda file or others
for (datafile in datafiles){  
  # appending full path to filename is necessary to open files in Rmd
  filename <- paste0("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses/",datafile) 
  load(file = filename)}


# create function to create one common legend for plots
get_legend<-function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}


# Clean single-trial data: Exclude word and GNG responses with misses or wrong keys, outliers in word RT or GNG RT (incorrect responses for RT LMMs are excluded when the (G)LMM is specified)
data4mixedmodels_words <- data4mixedmodels[data4mixedmodels$outlier_words == FALSE & data4mixedmodels$gng_resp <= 46 & data4mixedmodels$gng_invalid_rt == FALSE & data4mixedmodels$word_resp <= 54,]
# 14130 of 15480 trials left



# aggregating within subjects per condition (needed for ANOVAs)
df_rt_aggregated <- ddply(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], 
                          .(subjectID, condition), 
                          summarise,
                          rt = mean(word_rt),
                          rt_inverse = mean(word_rt_inverse)) 

data4mixedmodels_words$word_accuracy_num <- as.numeric(data4mixedmodels_words$word_accuracy) # to make this work, accuracy must be numeric, not factor

df_acc_aggregated <-   ddply(data4mixedmodels_words, 
                             .(subjectID, condition), 
                             summarise, 
                             correct_responses = sum(word_accuracy_num, na.rm = TRUE), 
                             total_number_responses = length(condition), 
                             percent_correct_responses = correct_responses/total_number_responses*100)

# get a single aggregated df and make variables factors
df_aggregated_per_subject               <- left_join(df_rt_aggregated,df_acc_aggregated,by = c("subjectID", "condition"))
df_aggregated_per_subject$response_type <- factor(substr(df_aggregated_per_subject$condition, 11, 12), levels=c("FA","FH","SH","CI"))
df_aggregated_per_subject$word_valence  <- factor(substr(df_aggregated_per_subject$condition, 1, 3))
df_aggregated_per_subject$subjectID     <- factor(df_aggregated_per_subject$subjectID)                              
  

# aggregating over subjects per condition and calculate SE/CI corrected for within design (only needed for barplots)
# I apply function to the non-aggregated (data4mixedmodels_words) data, as this better reflects the LMM approach (Julia also does it like that). Computing this with the aggregated data per subject (df_aggregated_per_subject) yields different results, as in the mean of the group means it would not be considered that subjects have different number of events per condition; this more resembles the ANOVA approach, because it assumes that each subject mean is equally reliable / relies on the same amount of information, which is not true

# The function summarySEwithinO.R also provides the unnormed means, Rmisc function summarySEwithin only the normed means
# sd, se, ci are identical - these are calculated based on the Morey-correction
# Neuros report unnormed means and this makes a bit more sense to me too -> I use the function summarySEwithinO.R
# See explanations here: 
# http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/     
# https://stackoverflow.com/questions/33980316/why-does-the-mean-differ-using-summarysewithin 


source("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses/functions/summarySEwithinO.R") # summarySEwithin function returning unnormed means provided by R-cookbook: http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/
df_aggregated_over_subjects_rt <- summarySEwithinO(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], measurevar="word_rt", withinvars="condition", idvar="subjectID", conf.interval=.95)
df_aggregated_over_subjects_rt$response_type <- factor(substr(df_aggregated_over_subjects_rt$condition, 11, 12), levels=c("FA","FH","SH","CI"))
df_aggregated_over_subjects_rt$word_valence  <- factor(substr(df_aggregated_over_subjects_rt$condition, 1, 3))

df_aggregated_over_subjects_acc <- summarySEwithinO(data4mixedmodels_words, measurevar="word_accuracy", withinvars="condition", idvar="subjectID", conf.interval=.95)
df_aggregated_over_subjects_acc$response_type <- factor(substr(df_aggregated_over_subjects_acc$condition, 11, 12), levels=c("FA","FH","SH","CI"))
df_aggregated_over_subjects_acc$word_valence  <- factor(substr(df_aggregated_over_subjects_acc$condition, 1, 3))


# long to wide format and calculate priming effect -> variables in df_wide are identical to those in df4save! I just avoid using df4save for the subsequent analyses... 
df_wide <-   reshape(data = df_aggregated_per_subject, 
                      direction = "wide",
                      v.names = c("rt","rt_inverse","percent_correct_responses","correct_responses"),
                      idvar = "subjectID",
                      timevar = "condition",
                      drop = c("subject","total_number_responses", "response_type","word_valence")) # variables to drop before reshaping

df_wide$rt_priming_after_FA       <- df_wide$rt.pos_after_FA - df_wide$rt.neg_after_FA
df_wide$rt_priming_after_FH       <- df_wide$rt.neg_after_FH - df_wide$rt.pos_after_FH
df_wide$accuracy_priming_after_FA <- df_wide$percent_correct_responses.neg_after_FA - df_wide$percent_correct_responses.pos_after_FA
df_wide$accuracy_priming_after_FH <- df_wide$percent_correct_responses.pos_after_FH - df_wide$percent_correct_responses.neg_after_FH

# calculate percent correct
df_wide <- data.frame(df_wide,ddply(data4mixedmodels_words,.(subjectID),summarise, 
                                    words_percent_correct_overall = sum(word_accuracy_num, na.rm = TRUE)/length(subjectID)*100,
                                    gng_error_rate = sum(response_type == "FA") / length(subjectID)*100))

# reorder columns
df_wide <- df_wide[,c("subjectID","rt_priming_after_FA","rt_priming_after_FH","rt.neg_after_FA","rt.pos_after_FA","rt.neg_after_FH","rt.pos_after_FH","rt.neg_after_SH","rt.pos_after_SH","rt.neg_after_CI","rt.pos_after_CI","rt_inverse.neg_after_FA","rt_inverse.pos_after_FA","rt_inverse.neg_after_FH","rt_inverse.pos_after_FH","rt_inverse.neg_after_SH","rt_inverse.pos_after_SH","rt_inverse.neg_after_CI","rt_inverse.pos_after_CI","accuracy_priming_after_FA","accuracy_priming_after_FH","percent_correct_responses.neg_after_FA","percent_correct_responses.pos_after_FA","percent_correct_responses.neg_after_FH","percent_correct_responses.pos_after_FH","percent_correct_responses.neg_after_SH","percent_correct_responses.pos_after_SH","percent_correct_responses.neg_after_CI","percent_correct_responses.pos_after_CI","words_percent_correct_overall","correct_responses.neg_after_FA","correct_responses.pos_after_FA","correct_responses.neg_after_FH","correct_responses.pos_after_FH","correct_responses.neg_after_SH","correct_responses.pos_after_SH","correct_responses.neg_after_CI","correct_responses.pos_after_CI","gng_error_rate")]


# get descriptive statistics
descriptive_statistics <- stat.desc(df_wide,basic=F)

# save aggregated df
# setwd("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses")    
# date_time <- format(Sys.time(), "%Y-%m-%d_%H-%M-%S")
# filename_aggregated <- paste("Data_Aggregated_For_",nrow(df_wide),"_subjects_",date_time, ".rda", sep = "")
# save(df_wide,file=filename_aggregated) 

```

<br> 
<br>
<br>

# Outlier Definition

For the word categorization, RTs 3 Median Absolute Deviations above/below the median of each condition for each subject were excluded.
For the GNG response, RTs < 100 ms and RTs > 700 ms were excluded.
This procedure is similar to the one used by Aarts (2.5 SD for word categorization and <100/>500 ms for GNG response.)
The data loss due to outlier exclusion is small.

NOTE: The used approach for outlier denifition may not be the best strategy, for 3 reasons:

1. For defining the outlier threshold (MAD) I here excluded word responses after invalid gng RT
<br> 
  * this could be criticized, but is not so problematic that I need to change it now after all analysis were reported (we could argue that after GNG invalid RTs the word response is also not normal) -> do not change this anymore, as this would change all outputs
<br> 
2. The incorrectly categorized words are automatically defined as non-outliers (not sure whether this is good or not)
<br> 
  * this is not problematic for all (RT) analyses, as I exclude these trials for these analyses anyway 
<br> 
  * but I do NOT exclude incorrect words for Accuracy analysis (GLMM) - so it may DO a bit of a difference there, right? But I still think that defining outliers among incorrect responses isn't really necessary either, as these responses are already incorrect, no matter what their RT is
<br> 
  * for calculation of proportion word outliers it might actually be good, because there I want to see how many among the correctly categorized words were outliers, right?
<br> 
3. I here apply the threshold (MAD) calculated for words after SHs also to the category words after Miss_or_False_Key 
<br> 
  * this is not problematic, as I exclude these trials for all analyses
<br> 
  * for calculation of proportion word outliers it might actually be good, because in the Miss_or_Wrong_Key category there are not enough events to calculate a reliable threshold, but in the SH there are enough events, as this is the biggest category
   
<br>

```{r excluded outliers}
# use all trials here, as recommended by JK
data4mixedmodels$outlier_words_pos <- FALSE
data4mixedmodels[data4mixedmodels$outlier_words == TRUE & data4mixedmodels$valence == "pos",]$outlier_words_pos <- TRUE

data4mixedmodels$outlier_words_neg <- FALSE
data4mixedmodels[data4mixedmodels$outlier_words == TRUE & data4mixedmodels$valence == "neg",]$outlier_words_neg <- TRUE


df_outlier <- ddply(data4mixedmodels, 
                          .(subjectID), 
                          summarise,
                          percent_word_outliers_pos = sum(outlier_words_pos)/length(subjectID)*100,
                          percent_word_outliers_neg = sum(outlier_words_neg)/length(subjectID)*100,
                          percent_word_outliers_overall = sum(outlier_words)/length(subjectID)*100)

outlier <- round(stat.desc(df_outlier[,c(2:4)]), digits = 2)

kable(outlier[c(4,5,9,13),], caption = "RT Word Outlier (%)") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "left")
```

<br> 
<br>
<br>

# Calculate FH Time

FH time is 80 % of mean RT of correct responses in training 1 and 90 % of mean RT of correct responses in training 2 and 3.
<br>

```{r FH time}
df_rt_training_1 <- ddply(data4mixedmodels_words[data4mixedmodels_words$gng_resp<=42 & data4mixedmodels_words$trial <= 28,], 
                       .(subjectID), 
                       summarise,
                       rt_1 = mean(gng_rt),
                       FH_1 = rt_1 - ((rt_1 * 20) / 100)) 
df_rt_training_2 <- ddply(data4mixedmodels_words[data4mixedmodels_words$gng_resp<=42 & (data4mixedmodels_words$trial >= 173 &  data4mixedmodels_words$trial <= 200),], 
                       .(subjectID), 
                       summarise,
                       rt_2 = mean(gng_rt),
                       FH_2 = rt_2 - ((rt_2 * 10) / 100)) 
df_rt_training_3 <- ddply(data4mixedmodels_words[data4mixedmodels_words$gng_resp<=42 & (data4mixedmodels_words$trial >= 345 &  data4mixedmodels_words$trial <= 372),], 
                       .(subjectID), 
                       summarise,
                       rt_3 = mean(gng_rt),
                       FH_3 = rt_3 - ((rt_3 * 10) / 100)) 
FH_mean  <- (mean(df_rt_training_1$FH_1) + mean(df_rt_training_2$FH_2) + mean(df_rt_training_3$FH_3)) / 3
FH_sd    <- (sd(df_rt_training_1$FH_1)   + sd(df_rt_training_2$FH_2)   + sd(df_rt_training_3$FH_3))   / 3
FH_range <- c(min(c(range(df_rt_training_1$FH_1),range(df_rt_training_2$FH_2),range(df_rt_training_3$FH_3))), max(c(range(df_rt_training_1$FH_1),range(df_rt_training_2$FH_2),range(df_rt_training_3$FH_3))))

paste("FH mean:" , round(FH_mean, digits=2), "ms")
paste("FH SD:   "   , round(FH_sd,   digits=2), "ms")
```

<br>
<br>
<br>

# Data Inspection

<br>

#### Visualize RT per Subject & Word

<br>

```{r plot word RT per subject and word, fig.width=10, fig.height=8}

per_subj <- ggplot(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], aes(x=response_type, y=word_rt)) + geom_point(position="jitter", aes(colour = valence)) + 
ggtitle("Word Categorization RT per Subject") + theme(plot.title = element_text(hjust = 0.5), legend.position="bottom") + facet_wrap( ~ subjectID, nrow=5)
per_subj

per_word <- ggplot(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], aes(x=response_type, y=word_rt)) + geom_point(position="jitter") + 
  ggtitle("Word Categorization RT per Word") + theme(plot.title = element_text(hjust = 0.5)) + facet_wrap( ~ word, nrow=10)
per_word
```

<br>

#### Distribution RT & Accuracy

I decided to transform the RT data, because they severely deviate from normal distribution. **Normal distribution of the residuals in the different conditions is a prerequisite for ANOVAs and LMMs** and violation has been found to have an effect on the probability of Type I error and the power. According to method papers, inverse transformation is better than the widely used log transformation to achieve normality and increase power. Checking my data with a *boxcox* function, the lambda (-0.83, so close to -1) also suggests that inverse transformation (1/RT) is the best option for my data. I also tested the log transformation. Visually, this leads to a comparable normalization of the data. I still chose the inverse transformation, as here the skewness is a bit smaller and the inverse transformation is assumed to yield better power. Based on the paper "A linear mixed model analysis of masked repetition priming" by Kliegl (2010), I used the reciprocal version of inverse transformation (-1/RT), as this transformation affords an interpretation of effects in terms of rate or speed rather than time. I  multiplied reciprocal scores by minus 1 to maintain the direction of effects compatible to the raw values, effectively converting speed into “rate of slowing”. Instead of -1/RT, I calculated -1000/RT, to avoid having very small decimal numbers. I multiplied the inversely transformed RTs by -1000 so that coefficients will have the same sign as for models fitted to the untransformed latencies, at the same time avoiding very small values and too restricted range for the dependent variable (from Baayen 2010 Analyzing Reaction Times).

I decided to exclude outliers before the transformation. I think that outlier exclusion is reasonable for the word RT from a theoretical point of view and it contributed to achieving a normal distribution (only transforming without excluding outliers still resulted in a non-normal distribution). As can be seen in the plots, the transformed RT data have approximately normal distribution. I will use the transformed RT for the ANOVA and the LMM.

Accuracy data (and residuals of accuracy in the different conditions) are not normally distributed and there is no transformation that leads to normality (see below). Aggregated accuracy (percent correct in the different conditions) will be analyzed with an ANOVA anyway, as no suitable non-parametric alternative is available. The correctness on single trial level will be analyzed with a GLMM, as such binary events have a binomial distribution. 


<br>

```{r inspect distribution word RT, fig.width=10, fig.height=8}

# overlay normal density function curve to see how closely it fits normal distribution for normal curve: important to add aes(y = .. density..) to geom_histogram
hist_word_rt = ggplot(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], aes(x=word_rt)) +
  geom_histogram(aes(y = ..density..),color="gray33", fill = "lightcoral", binwidth = 45, size = 1) +
  stat_function(fun=dnorm, args=list(mean=mean(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt,na.rm = TRUE), sd=sd(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt,na.rm = TRUE)), color="black", size = 0.5) +
  geom_vline(aes(xintercept=mean(word_rt)), color="black", linetype="dashed", size= 1) +
  labs (title = "Histogram Word RT Raw", x = "Word RT", y ="Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_word_rt = ggplot(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], aes(sample=word_rt)) +
  stat_qq(color = "lightcoral") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Word RT Raw", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))

hist_word_rt_inverse = ggplot(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], aes(x=word_rt_inverse)) +
  geom_histogram(aes(y = ..density..),color="gray33", fill = "lightcyan3", binwidth = 0.1, size = 1) +
  stat_function(fun=dnorm, args=list(mean=mean(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt_inverse,na.rm = TRUE), sd=sd(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt_inverse,na.rm = TRUE)), color="black", size = 0.5) +
  geom_vline(aes(xintercept=mean(word_rt_inverse)), color="black", linetype="dashed", size= 1) +
  labs (title = "Histogram Word RT Inverse", x = "Word RT Inverse", y ="Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_word_rt_inverse = ggplot(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], aes(sample=word_rt_inverse)) +
  stat_qq(color = "lightcyan3") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Word RT Inverse", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))

hist_word_accuracy = ggplot(df_wide, aes(x=words_percent_correct_overall)) +
  geom_histogram(aes(y = ..density..),color="gray33", fill = "lightgoldenrod", binwidth = 1, size = 1) +
  stat_function(fun=dnorm, args=list(mean=mean(df_wide$words_percent_correct_overall,na.rm = TRUE), sd=sd(df_wide$words_percent_correct_overall,na.rm = TRUE)), color="black", size = 0.5) +
  geom_vline(aes(xintercept=mean(words_percent_correct_overall)), color="black", linetype="dashed", size= 1) +
  labs (title = "Histogram Word Accuracy", x = "Word Accuracy", y ="Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_word_accuracy = ggplot(df_wide, aes(sample=words_percent_correct_overall)) +
  stat_qq(color = "lightgoldenrod") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Word Accuracy", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))


grid.arrange(hist_word_rt,qqplot_word_rt,
             hist_word_rt_inverse,qqplot_word_rt_inverse,
             hist_word_accuracy,qqplot_word_accuracy,nrow = 3)

# qqplots for individual subjects 
# qqplot_word_rt_subjects         <- qqmath(~word_rt | subject, data = data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,])
# qqplot_word_rt_inverse_subjects <- qqmath(~word_rt_inverse | subject, data = data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,])
# grid.arrange(qqplot_word_rt_subjects,qqplot_word_rt_inverse_subjects,nrow = 2) --- > still looks weird in html
```

<br>

#### Normality of Aggregated Data 

The Shapiro-Wilk test is used to test normality of the aggregated data. After transformation, the RT in all conditions over all subjects is normally distributed. The raw RT as well as the accuracy is not normally distributed in the conditions.

```{r test distribution word RT aggregated}

normality <- do.call(rbind, lapply(df_wide[,c("rt.neg_after_FA","rt.pos_after_FA","rt.neg_after_FH","rt.pos_after_FH", 
                                       "rt.neg_after_CI","rt.pos_after_CI","rt.neg_after_SH","rt.pos_after_SH","rt_inverse.neg_after_FA","rt_inverse.pos_after_FA",                                                                                   "rt_inverse.neg_after_FH","rt_inverse.pos_after_FH", 
                                       "rt_inverse.neg_after_CI","rt_inverse.pos_after_CI","rt_inverse.neg_after_SH","rt_inverse.pos_after_SH", 
                                       "percent_correct_responses.neg_after_FA","percent_correct_responses.pos_after_FA","percent_correct_responses.neg_after_FH", 
                                       "percent_correct_responses.pos_after_FH","percent_correct_responses.neg_after_CI","percent_correct_responses.pos_after_CI",
                                       "percent_correct_responses.neg_after_SH","percent_correct_responses.pos_after_SH")], function(x) shapiro.test(x)["p.value"]))


normality             <- unlist(normality[,1])
normality             <- data.frame(round(normality, digits = 2))
normality$condition   <- rownames(normality)
rownames(normality)   <- NULL
colnames(normality)   <- c("Shapiro_Wilk_p_value","Condition")
normality$Shapiro_Wilk_p_value[normality$Shapiro_Wilk_p_value < 0.001] <- "< 0.001"
normality <- cbind(normality[c(1:8),],normality[c(9:16),],normality[c(17:24),])

kable(normality) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")  %>%
  add_header_above(c("RT Normality before Transformation" = 2, "RT Normality after Transformation" = 2, "Accuracy Normality" = 2))
```

<br>

#### Normality of Single Trial RT Data

For the single-trial data, Shapiro-Wilk is not suitable, as it always returns a significant result for such large samples (additionally, it can handle only samples up to 5000). Hence, we have to rely on visual inspection (see above) and values of skewness and kurtosis. Values for skewness and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010).

```{r test distribution word RT single}

normality <- round(data.frame(matrix(c(skewness(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt),kurtosis(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt),skewness(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt_inverse),kurtosis(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt_inverse)),nrow=2,ncol=2)),digits = 1)
rownames(normality) <- c("Skewness","Kurtosis")
colnames(normality) <- c("Word RT Raw","Word RT transformed")

kable(normality) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "left")
```

<br>

#### Summary

Normal distribution of sampling data can be assumed for inverse word RT.
Normal distribution of sampling data **cannot** be assumed for untransformed word RT and for accuracy.




<br>
<br>
<br>

# LMM Choices

I used a linear mixed model to evaluate whether the response time and accuracy of the word categorization differs depending on type of the previous response to the GNG stimulus and the word valence. 
I thought about analyzing word categorization RT by means of a GLMM on the raw RTs instead of a LMM on the transformed RT. This is recommended by Lo & Anderson (2015) and more recent statistics literature, because transformation makes interpretation of model estimates more difficult, and it makes mischief with the variance of the transformed variable and the interaction effects. Follwing this recommendation, I also analyzed word categorization RT by means on a GLMM. Lo & Anderson (2015) recommend gamma or inverse gaussian distribution for RTs. For the word RTs, inverse gaussian distribution fits very well and better than gamma distribution. Hence, I chose inverse gaussian distribution and the identity link funktion (recommended by Lo & Andrews, 2015) for my GLMM. 
Unfortunately, convergence is much more difficult, especially for the inverse gaussian distribution, which fits our data best. The GLMM only converges with less random slopes than the LMM (but see below, this is not the case anymore! Convergence failure message was a false positive!). The effects of interest (interaction response type x valence) are the same in GLMM and LMM. The only difference is that in the GLMM there is also a significant difference between response types FH_vs_SH (in the full and the nested model). This maybe results from a type I error that may be increased due to the sparse random structure (this effect is also not significant in the ANOVA post hoc test; but it turns out, it is not an effect of the sparse random structure, as it is also present in the maximal GLMM). Further arguments favoring the LMM are that I can more flexibly included control variables (as GNG RT) and still have converging models and that it is consistent with the choice of an LMM to analyze SCR, which I did not manage to do with the GLMM due to the weird distribution of the SCR data. Hence, I decide to run LMMs (except for the binomial accuracy data) but I may report that parallel analyses conducted on raw RT using GLMM models that assumed an inverse gaussian distribution of residuals yielded the same outcomes.

**Overview (G)LMM:**

* Response variable: RT (inverse transformed) // Accuracy (correct vs incorrect categorization)

* Fixed effects: Response type (FA, FA, SH, CI) and Word Valence (pos, neg)
    
* Random effects: Subjects and Words

Trials with invalid RT to GNG stimulus or word were excluded from all analyses. FHs, SHs, FAs, CIs in the GNG task were included here. Trials with misses or responses with wrong keys in the GNG task or the word classification were not included. For LMMs on the RT, only trials with correct classification of the word were included. 

<br>

#### GNG RT / Error Rate as Covar.?

The response types differ in terms of RT: FA = FH, but FA and FH < SH. Hence, including **GNG RT** as a covariate might be a good idea. But several things are problematic when including GNG RT as covariate. The most important one:

* When I want to include CI in the contrasts and analysis, there is no GNG RT for these trials.
    * Assigning NaN to these trials leads to dropping of estimation for all CI main effects and interactions.
    * If I replace NaNs with 0, I get estimates for CI, but I am not sure in how far this messes up interpretability, as now there is a real value for no real event.

I prefer to not include GNG RT as covariate. Reliably testing priming (also after CI) is more important to me than having GNG RT as covariate. 

But I checked whether inclusion of this covariate may change the results. I included GNG RT as a covariate in the LMM on the transformed word RT inverse (and I also tested it in the GLMM on the raw RT). I did not include it as a random slope.

The results for the LMM (and GLMM) were exactly the same:
The 3-way interaction gng_rt_inverse x valence x response_type did not improve the model fit. There is a main effect of GNG RT but no interaction with valence x response_type. Hence, the priming effect is not modulated by the GNG RT. The model fit is a bit better when the covariate is included (both for the full and the nested model). 
The results of all main effects and interactions are the same, independent of whether the covariate is included or not.


I also checked **error rate**, **avoidance of errors**, **frustration over errors** and **effort** as a covariate (only in the LMM). The covariates were centered and only included as fixed effect, not as random slope. For avoidance of errors and frustration over errors there might be a ceiling effect, as all subjects rated these variables quite high
For all of these covariates, the 3-way interaction cov x valence x response_type was not significant and did not improve the model fit. In the nested model, the priming effect after FA, FH, SH remains significant. The priming effect (response_type x valence or response_type/valence) remained significant in all full and nested models, for no response type there was a significant interaction valence x cov in any nested model. I can conclude that the priming effect is not modulated by any of these variables. 


<br>

#### Check Distribution 

Word RT is not normally distributed (see above). The transfomed Word RT Inverse is normally distributed, allowing data analysis by means of an LMM (the residuals of this LMM are normally distributed as well). 

```{r distribution LMM RT, fig.width=10, fig.height=6}

# distribution transformed rt -> normally distributed, also in the different conditions
par(mfrow = c(1,2))  # use par function in base graphics approach to arrange plots side by side here; grid.arrange only works for ggplot grahpics
plot(density(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt_inverse), main = "Histogram Word RT Inverse")
qqp(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$word_rt_inverse, "norm", main = "Q-Q Plot Word RT Inverse", ylab = "sample quantiles", id = FALSE)
par(mfrow = c(1, 1)) # reset "par" parameter
ggplot(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], aes(x = word_rt_inverse)) + geom_density() + facet_wrap(response_type ~ valence) + ggtitle("Histogram Word RT Inverse in Conditions")
```
<br>


Accuracy is a binary variable (TRUE / FALSE, 0/1, presence / absence data) and thus follows a binomial distribution.

<br>

#### Define Contrasts

I chose sliding difference (aka "repeated") contrasts, which successively test neighboring factor levels against each other. The resulting estimates can be interpreted as the difference between subsequent factor levels. The advantage of this contrast is that the fixed effect intercept (group-level mean) is estimated as the grand average across all conditions, rather than the mean of a baseline condition, as for example for the default treatment contrast, which can cause troubles when using multiple predictors or interactions. Sliding constrasts are centered, meaning that the intercept reflects the grand mean. This is really important if the factors have more than 2 levels and when interactions between levels are tested. Compared to treatment contrasts, sliding contrasts allow better interpretation of interactions. For these contrasts, the predictor variables actually measure their main effects, i.e., their effects AVERAGED across the conditions of the other factor. I decided to compare FH with SH, FA with FH, and CI with FA (we can only compare n-1 factor levels in each model). 

I could also use custom contrast to build sliding contrast for response condition and then directly test valence effect nested in each response condition. I tested this, my custom contrasts investigating directly the nested interaction give the same result as the nested model derived from the full interaction model with sliding contrasts. This custom interaction contrast fits my hypotheses and tests them directly instead of calculating a model with the interaction valence x response type and then breaking down the interaction. However, the custom interaction contrast gives no estimate for the effect of word valence and I want to show that there is no such main effect. Thus, I decided to first calculate the interaction model and then nested model to break down the interaction. Also other people with expterise in this field who were interested in the nested interaction first run the full model and then the nested model. I may put the full model in the supplement, if it is too much to report all models. Daniel Schad slightly favored to directly test the hypotheses, so the nested model first, then the full model maybe as a supplement for the valence effect. But either way might be fine. 

```{r contrasts (G)LMM Words}
# make categorical variables factors
data4mixedmodels_words$response_type <- factor(data4mixedmodels_words$response_type, levels=c("SH","FH","FA","CI"))
data4mixedmodels_words$valence       <- factor(data4mixedmodels_words$valence)
data4mixedmodels_words$condition     <- factor(data4mixedmodels_words$condition, levels=c("neg_after_SH", "pos_after_SH", "neg_after_FH","pos_after_FH", "neg_after_FA","pos_after_FA", "neg_after_CI", "pos_after_CI"))
data4mixedmodels_words$subjectID     <- factor(data4mixedmodels_words$subjectID)
data4mixedmodels_words$word          <- factor(data4mixedmodels_words$word)
data4mixedmodels_words$word_accuracy <- factor(data4mixedmodels_words$word_accuracy)

# define contrasts
contrasts(data4mixedmodels_words$response_type) <- contr.sdif(4)
contrasts(data4mixedmodels_words$valence)       <- contr.sdif(2)

# add contrast as numerical covariate via model matrix (in order to enter contrasts individually in model and hence to use ||)
mm_c    <- model.matrix( ~ response_type*valence, data4mixedmodels_words) # stick in model matrix 8 columns

# attach to dataframe 
data4mixedmodels_words[,(ncol(data4mixedmodels_words)+1):(ncol(data4mixedmodels_words)+8)] <- mm_c
names(data4mixedmodels_words)[(ncol(data4mixedmodels_words)-7):ncol(data4mixedmodels_words)] <- c("Grand Mean", "FH_minus_SH","FA_minus_FH","CI_minus_FA","pos_minus_neg", "FH_minus_SH:pos_minus_neg", "FA_minus_FH:pos_minus_neg", "CI_minus_FA:pos_minus_neg")

# prepare labels for (G)LMM tables
pl <- c(
  "(Intercept)" = "Intercept",
  "valence2-1" = "Positive - Negative",
  "response_type2-1" = "FH - SH", 
  "response_type3-2" = "FA - FH",
  "response_type4-3" = "CI - FA",
  "response_type2-1:valence2-1" = "FH - SH x Positive - Negative", 
  "response_type3-2:valence2-1" = "FA - FH x Positive - Negative",
  "response_type4-3:valence2-1" = "CI - FA x Positive - Negative",
  "response_typeSH:valence2-1" = "SH: Positive - Negative",
  "response_typeFH:valence2-1" = "FH: Positive - Negative",
  "response_typeFA:valence2-1" = "FA: Positive - Negative",
  "response_typeCI:valence2-1" = "CI: Positive - Negative",
  "pos_minus_neg" = "Positive - Negative",
  "FH_minus_SH" = "FH - SH", 
  "FA_minus_FH" = "FA - FH",
  "CI_minus_FA" = "CI - FA",
  "FH_minus_SH:pos_minus_neg" = "FH - SH x Positive - Negative", 
  "FA_minus_FH:pos_minus_neg" = "FA - FH x Positive - Negative",
  "CI_minus_FA:pos_minus_neg" = "CI - FA x Positive - Negative")
```

<br>

#### Model Building

* I always started with a model including the maximal random structure. 
* (When the model was degenerate (singular fit),) I reduced the random effects structure by running a principal component analysis and excluded random-effects variance-covariance estimates and correlation parameters until the random structure was supported and convergence achieved (Barr, Levy, Scheepers, & Tily, 2013; Bates, Kliegl, Vasishth, & Baayen, 2015; Singmann & Kellen, n.d.).
    * For that, I always started with first removing the correlation parameters between random slopes and intercepts (using the double bar syntax), then all other random parameters that explained zero variance till an identified, not degenerate model was achieved.
* I presented the final model by using the REML estimation, as ML may underestimate variance of the random effects.
    * The parameter estimates were obtained using the restricted maximum likelihood (REML) estimation method which produces relatively unbiased estimates. However, the Akaike information criterion (AIC) and log-likelihood values can only be obtained by using the same models with the maximum likelihood (ML) estimation method. 



<br>

# LMM RT 

<br>

#### Run LMM

For the Word Categorization RT, the  maximal random effects were reduced of the random effect explaining zero variance “CI - FA” for word; with correlation parameters set to zero. Note that the model using the maximal random structure (i.e., including “CI - FA”) also converged and yielded the same estimates. As long as the maximal model were to yield the same estimates as a justifiably reduced parsimonious model, there actually would be no need for model reduction (Matuschek 2017). For sake of simple explainability of the model selection, I decided to maintain the procedure to remove zero variance components. 


	* **GENERAL REMARK: the tab_model function gives this nicely formatted table. But: It uses Wald approximation for calculating dfs and *p* values. This is not the recommended method. So, either add p.val = "kr" to receive Kenward-Roger approximation (most recommended but takes ages) or take *p* values based on Satterthwaite approximation (also ok) from summary(model) command. To get correct values for marginal and conditional R², all fixed effects have to be entered as separate contrasts. I did not manage to do this for the nested models. Reported R² is derived from the full models (values for the nested models is different).**


```{r build LMM RT, echo = 2}

LMM_rt_final <- lmer(word_rt_inverse ~ FH_minus_SH + FA_minus_FH + CI_minus_FA + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + CI_minus_FA:pos_minus_neg + (1 + FH_minus_SH + FA_minus_FH + CI_minus_FA + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + CI_minus_FA:pos_minus_neg || subjectID) + (1 + FH_minus_SH + FA_minus_FH || word), data=data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

tab_model(LMM_rt_final, dv.labels = "RT [-1000/RT(ms)]] Word Categorization", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)
```
<br>

Output Interpretation: The intercept reflects the grand mean of all conditions. The slopes express the difference between the conditions. The slope estimate is influenced by which other fixed and random effects are included in the model.
(Akin to the ANOVA omnibus test, it can be tested whether the interaction response type x valence improves the model fit significantly. It does. Thus, there is an omnibus interaction effect.) The interaction valence x response type was resolved in the next step.

<br>

#### Break Down the Interaction

I tested the effect of valence within each response condition by calculating a LMM that is nested in the final model above. 

```{r nested LMM RT, echo = 2}
LMM_rt_nested <- lmer(word_rt_inverse ~ response_type/valence + (1 + FH_minus_SH + FA_minus_FH + CI_minus_FA + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + CI_minus_FA:pos_minus_neg || subjectID) + (1 + FH_minus_SH + FA_minus_FH || word), data=data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

tab_model(LMM_rt_nested,dv.labels = "RT [-1000/RT(ms)]] Word Categorization", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)
```

<br>
<br>


#### Summary LMM

For RT, there is a priming effect after FH, SH, FA but not after CI. There is no effect of word valence. An effect of response type is evident for FH vs FA. These results are consistent with the ANOVA post-hoc tests. 

The results are also in line with the GLMM on the raw word RT. The only difference is that in the GLMM there is also a significant difference between response types FH_vs_SH. This is of no interest. Due to the presence of a disordinal interaction effect, the main effects cannot be sensibly interpreted anyway.


<br>

#### Check Assumptions LMM

Assumption 1: Linearity 
* Dependent variable linearly related to the fixed factors, random factors, and covariates 
* Plot of the residuals against the fitted values  shows a random scatter pattern, no nonlinear or curvy pattern -> linearity given

Assumption 2: Homogeneity of Variance 
* Residuals have constant variance across the range of your predicted values
* Plot of the residuals against the fitted values shows an even spread around the centered line -> homogeneity of variance given
    
Assumption 3: Normality of Residuals 
* Residuals are approximately normally distributed.
* Q-Q plot looks fine; a bit off at the extremes, but that's often the case; doesn't look too bad -> normality given

```{r test LMM assumptions, fig.width=10, fig.height=4, fig.align='center'} 
par(mfrow = c(1,2))  # use par function in base graphics approach to arrange plots side by side here; grid.arrange only works for ggplot grahpics

plot(fitted(LMM_rt_final), residuals(LMM_rt_final), xlab = "Fitted Values", ylab = "Residuals", main = "Residuals versus Predicted Variable")  # plot residuals against the fitted values
abline(h = 0, lty = 2)
lines(smooth.spline(fitted(LMM_rt_final), residuals(LMM_rt_final)))

qqnorm(resid(LMM_rt_final))
qqline(resid(LMM_rt_final)) 

par(mfrow = c(1, 1)) # reset "par" parameter

skewness <- round(skewness(resid(LMM_rt_final)),digits = 2) 
kurtosis <- round(kurtosis(resid(LMM_rt_final)),digits = 2) 

print(paste("Skewness Residuals Distribution:", skewness))
print(paste("Kurtosis Residuals Distribution:", kurtosis))
```


  

<br>
<br>

#### Output GLMM RT

I only print the output here, as running these two models takes ages. As mentionned, compared the the LMM there is an additional main effect of FH-SH. Previously I thought that this was due to the reduced random structure in the GLMM. I now realized that convergence errors were false positives here and I can very well use the full random structure in the full and nested model. Even with this structure, the effect FH - SH is still present.
For checks of distribution fit and assumptions, see GLMM script.
**NOTE: When trying to replicate this GLMM for code publishing, I did not manage to obtain the same results. This model did not converge, so I needed to remove the random slope FA − FH for word stimuli, leading to slightly different estimates. These are now reported in the manuscript.**

![GLMM RT full](P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses/GLMM_rt_full.PNG)
![GLMM RT full](P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses/GLMM_rt_nested.PNG)

```{rt GLMM, echo = TRUE} 
# GLMM_rt_full <- glmer(word_rt ~ response_type*valence + (1 + FH_minus_SH + FA_minus_FH + CI_minus_FA + pos_minus_neg + FH_minus_SH:pos_minus_neg + # FA_minus_FH:pos_minus_neg + CI_minus_FA:pos_minus_neg || subjectID) + (1 + FH_minus_SH + FA_minus_FH + CI_minus_FA || word), # data=data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], family = inverse.gaussian(link="identity"), # control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e6)))
# 
# Random effects:
#  Groups      Name                      Variance      Std.Dev. 
#  word        CI_minus_FA                118.43424283 10.882750
#  word.1      FA_minus_FH                 44.51097843  6.671655
#  word.2      FH_minus_SH                126.29909438 11.238287
#  word.3      (Intercept)                150.41684315 12.264454
#  subjectID   CI_minus_FA:pos_minus_neg 2818.45443900 53.089118
#  subjectID.1 FA_minus_FH:pos_minus_neg 2334.53748978 48.317052
#  subjectID.2 FH_minus_SH:pos_minus_neg  878.35974148 29.637135
#  subjectID.3 pos_minus_neg              360.57987588 18.988941
#  subjectID.4 CI_minus_FA               1732.31263609 41.621060
#  subjectID.5 FA_minus_FH               1761.98067884 41.975954
#  subjectID.6 FH_minus_SH                306.98228559 17.520910
#  subjectID.7 (Intercept)               1314.52987837 36.256446
#  Residual                                 0.00005594  0.007479
# Number of obs: 13092, groups:  word, 60; subjectID, 30
# 
# Fixed effects:
#                             Estimate Std. Error t value             Pr(>|z|)    
# (Intercept)                  749.114      6.953 107.743 < 0.0000000000000002 ***
# response_type2-1              -8.758      4.253  -2.059              0.03947 *  
# response_type3-2             121.192      9.019  13.437 < 0.0000000000000002 ***
# response_type4-3              -4.145      9.229  -0.449              0.65336    
# valence2-1                    14.778      5.407   2.733              0.00628 ** 
# response_type2-1:valence2-1  -10.324      7.755  -1.331              0.18306    
# response_type3-2:valence2-1  136.815     12.612  10.848 < 0.0000000000000002 ***
# response_type4-3:valence2-1 -106.498     13.900  -7.662   0.0000000000000183 ***
# 
# Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
# 
# The relative maximum gradient of 0.000569 is less than our 0.001 criterion.
# You can safely ignore any warnings about a claimed convergence failure.
# rePCA: All items explain variance.
# 
# 
# 
# GLMM_rt_nested <- glmer(word_rt ~ response_type/valence + (1 + FH_minus_SH + FA_minus_FH + CI_minus_FA + pos_minus_neg + FH_minus_SH:pos_minus_neg + # FA_minus_FH:pos_minus_neg + CI_minus_FA:pos_minus_neg || subjectID) + (1 + FH_minus_SH + FA_minus_FH + CI_minus_FA || word), # data=data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], family = inverse.gaussian(link="identity"), # control=glmerControl(optimizer="bobyqa"),optCtrl=list(maxfun=2e6))
# 
# Random effects:
#  Groups      Name                      Variance      Std.Dev. 
#  word        CI_minus_FA                118.44137955 10.883078
#  word.1      FA_minus_FH                 44.50804874  6.671435
#  word.2      FH_minus_SH                126.30221011 11.238426
#  word.3      (Intercept)                150.41828612 12.264513
#  subjectID   CI_minus_FA:pos_minus_neg 2818.55522461 53.090067
#  subjectID.1 FA_minus_FH:pos_minus_neg 2334.45855897 48.316235
#  subjectID.2 FH_minus_SH:pos_minus_neg  878.28926064 29.635945
#  subjectID.3 pos_minus_neg              360.57631738 18.988847
#  subjectID.4 CI_minus_FA               1732.28438101 41.620721
#  subjectID.5 FA_minus_FH               1762.00864312 41.976287
#  subjectID.6 FH_minus_SH                306.98981728 17.521125
#  subjectID.7 (Intercept)               1314.54413753 36.256643
#  Residual                                 0.00005594  0.007479
# Number of obs: 13092, groups:  word, 60; subjectID, 30
# 
# Fixed effects:
#                            Estimate Std. Error t value             Pr(>|z|)    
# (Intercept)                749.1161     6.9528 107.743 < 0.0000000000000002 ***
# response_type2-1            -8.7582     4.2530  -2.059              0.03947 *  
# response_type3-2           121.1915     9.0193  13.437 < 0.0000000000000002 ***
# response_type4-3            -4.1451     9.2292  -0.449              0.65334    
# response_typeSH:valence2-1 -19.2615     8.9038  -2.163              0.03052 *  
# response_typeFH:valence2-1 -29.5857     8.0326  -3.683              0.00023 ***
# response_typeFA:valence2-1 107.2292    10.7707   9.956 < 0.0000000000000002 ***
# response_typeCI:valence2-1   0.7311    11.3400   0.064              0.94859    
# 
# Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
# 
# The relative maximum gradient of 0.0000836 is less than our 0.001 criterion.
# You can safely ignore any warnings about a claimed convergence failure.
# rePCA: All items explain variance.
```


<br>
<br>

# GLMM Accuracy 

<br>

#### Run GLMM

For the Word Categorization Accuracy, the  maximal random effects were reduced of the random effect explaining zero variance “FA - FH” for subject; with correlation parameters set to zero. Note that the model using the maximal random structure (i.e., including “FA - FH”) also converged and yielded the same estimates (some authors prefer to have lower-order effects included as long as higher-order effects are included). As long as the maximal model were to yield the same estimates as a justifiably reduced parsimonious model, there actually would be no need for model reduction (Matuschek 2017). For sake of simple explainability of the model selection, I decided to maintain the procedure to remove zero variance components, even if they are lower-oder effects when higher-order effects are still included. 

```{r build GLMM Acc, echo = 2}

GLMM_acc_final <- glmer(word_accuracy ~ FH_minus_SH + FA_minus_FH + CI_minus_FA + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + CI_minus_FA:pos_minus_neg + (1 + FH_minus_SH + CI_minus_FA + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + CI_minus_FA:pos_minus_neg || subjectID) + (1 + FH_minus_SH + FA_minus_FH + CI_minus_FA || word), data=data4mixedmodels_words, family = binomial, control=glmerControl(optimizer="bobyqa"))

tab_model(GLMM_acc_final,dv.labels = "Correctness of Word Categorization", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)
```
<br>

Akin to the ANOVA omnibus test, it can be tested whehter the interaction response type x valence improves the model fit significantly. It does. Thus, there is an omnibus interaction effect. This interaction was resolved in the next step.

<br>

#### Break Down the Interaction

I tested the effect of valence within each response condition by calculating a GLMM that is nested in the final model above. 

```{r nested GLMM Acc, echo = 2}
GLMM_acc_nested <- glmer(word_accuracy ~ response_type/valence + (1 + FH_minus_SH + CI_minus_FA + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + CI_minus_FA:pos_minus_neg || subjectID) + (1 + FH_minus_SH + FA_minus_FH + CI_minus_FA || word), data=data4mixedmodels_words, family = binomial, control=glmerControl(optimizer="bobyqa"))

tab_model(GLMM_acc_nested,dv.labels = "Correctness of Word Categorization", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)
```

<br>

#### Summary GLMM

For accuracy, there is a priming effect after FA but not after FH, SH, or CI. An effect of response type for FH vs FA and FA vs CI and an effect of valence pos vs neg is evident. Due to the presence of an interaction, the main effects will not be interpreted. These results are consistent with the ANOVA post-hoc tests. 



<br>

#### Check Assumptions GLMM

For GLMMs, testing assumptions is not straightforward, see: https://www.theanalysisfactor.com/regression-diagnostics-glmm/

* Assumption 1: The chosen link function is appropriate 
    * Is ok
<br>    
* Assumption 2: Random effects (intercepts and slopes) are normally distributed
    * Is only partly ok. As can be seen in the Q-Q plot, there are 3 outliers that lead to deviation from normal distribution of random intercept of words.
    * Inspection of the random effect revealed that the words "einsam", "herzlos", and "lieblos" are these outliers (these words were related to slow responses). 
    * For the random intercept for words, the Shapiro-Wilk test is significant. It is also significant for the random slope pos_minus_neg and CI_minus_FA:pos_minus_neg for subjects, but this random slope only seems to mildly deviate from normal distribution (subjects 3 and 4 / subject 5 are mild outliers here).

```{r test GLMM assumption 2, fig.width=10, fig.height=3} 
par(mfrow = c(1,3))  # use par function in base graphics approach to arrange plots side by side here; grid.arrange only works for ggplot grahpics

r_int_subjects <- ranef(GLMM_acc_final)$subjectID$`(Intercept)`
qqnorm(r_int_subjects, main = "Q-Q Plot Random Intercept Subjects")
qqline(r_int_subjects)
shapiro_int_subjects <- shapiro.test(r_int_subjects)["p.value"]

r_slope1_subjects <- ranef(GLMM_acc_final)$subjectID$'FH_minus_SH'
qqnorm(r_slope1_subjects, main = "Q-Q Plot Slope FH_minus_SH")
qqline(r_slope1_subjects)
shapiro_slope1_subjects <- shapiro.test(r_slope1_subjects)["p.value"] 

r_slope2_subjects <- ranef(GLMM_acc_final)$subjectID$'CI_minus_FA'
qqnorm(r_slope2_subjects, main = "Q-Q Plot Slope CI_minus_FA")
qqline(r_slope2_subjects)
shapiro_slope2_subjects <- shapiro.test(r_slope2_subjects)["p.value"] 

r_slope3_subjects <- ranef(GLMM_acc_final)$subjectID$pos_minus_neg
qqnorm(r_slope3_subjects, main = "Q-Q Plot Slope pos_minus_neg")
qqline(r_slope3_subjects)
shapiro_slope3_subjects <- shapiro.test(r_slope3_subjects)["p.value"] 
# ranef(GLMM_acc_final)$subjectID$pos_minus_neg: 2 subjects (Ss 3 and 4) are mild outliers; rest is normally distributed, but Shapiro is sign., but only with 0.04

r_slope4_subjects <- ranef(GLMM_acc_final)$subjectID$'FH_minus_SH:pos_minus_neg'
qqnorm(r_slope4_subjects, main = "Q-Q Plot Slope FH_minus_SH:pos_minus_neg")
qqline(r_slope4_subjects)
shapiro_slope4_subjects <- shapiro.test(r_slope4_subjects)["p.value"] 

r_slope5_subjects <- ranef(GLMM_acc_final)$subjectID$'pos_minus_neg:FA_minus_FH'
qqnorm(r_slope5_subjects, main = "Q-Q Plot Slope FA_minus_FH:pos_minus_neg")
qqline(r_slope5_subjects)
shapiro_slope5_subjects <- shapiro.test(r_slope5_subjects)["p.value"] 

r_slope6_subjects <- ranef(GLMM_acc_final)$subjectID$'CI_minus_FA:pos_minus_neg'
qqnorm(r_slope6_subjects, main = "Q-Q Plot Slope CI_minus_FA:pos_minus_neg")
qqline(r_slope6_subjects)
shapiro_slope6_subjects <- shapiro.test(r_slope6_subjects)["p.value"] 
# ranef(GLMM_acc_final)$subjectID$'CI_minus_FA:pos_minus_neg': 1 subject (Ss 5) is outlier; rest is normally distributed, but Shapiro is sign., but only with 0.04

r_int_words <- ranef(GLMM_acc_final)$word$`(Intercept)`
qqnorm(r_int_words, main = "Q-Q Plot Random Intercept Words")
qqline(r_int_words)
shapiro_int_words <- shapiro.test(r_int_words)["p.value"] 
#ranef(GLMM_acc_final)$word$`(Intercept)`: 3 words (einsam, herzlos, lieblos) are outliers; rest is normally distributed, but Shapiro is sign.

r_slope1_words <- ranef(GLMM_acc_final)$word$FH_minus_SH
qqnorm(r_slope1_words, main = "Q-Q Plot Slope FH_minus_SH")
qqline(r_slope1_words)
shapiro_slope1_words <- shapiro.test(r_slope1_words)["p.value"] 

r_slope2_words <- ranef(GLMM_acc_final)$word$FA_minus_FH
qqnorm(r_slope2_words, main = "Q-Q Plot Slope FA_minus_FH")
qqline(r_slope2_words)
shapiro_slope2_words <- shapiro.test(r_slope2_words)["p.value"] 

r_slope3_words <- ranef(GLMM_acc_final)$word$CI_minus_FA
qqnorm(r_slope3_words, main = "Q-Q Plot Slope CI_minus_FA")
qqline(r_slope3_words)
shapiro_slope3_words <- shapiro.test(r_slope3_words)["p.value"] 

par(mfrow = c(1, 1)) # reset "par" parameter

rand_effects_shapiro <- data.frame(round(c(unlist(shapiro_int_subjects),unlist(shapiro_slope1_subjects),unlist(shapiro_slope2_subjects),unlist(shapiro_slope3_subjects),unlist(shapiro_slope4_subjects),unlist(shapiro_slope5_subjects),unlist(shapiro_slope6_subjects),unlist(shapiro_int_words),unlist(shapiro_slope1_words),unlist(shapiro_slope2_words),unlist(shapiro_slope3_words)), digits = 4))
rownames(rand_effects_shapiro) <- c("Random Intercept Subjects","Random Slope Subjects FH_minus_SH","Random Slope Subjects CI_minus_FA","Random Slope Subjects pos_minus_neg", "Random Slope Subjects FH_minus_SH:pos_minus_neg", "Random Slope Subjects FA_minus_FH:pos_minus_neg", "Random Slope Subjects CI_minus_FA:pos_minus_neg", "Random Intercept Words","Random Slope Words FH_minus_SH","Random Slope FA_minus_FH","Random Slope Words CI_minus_FA")
colnames(rand_effects_shapiro) <- "Shapiro-Wilk p Value"
rand_effects_shapiro <- t(rand_effects_shapiro)

kable(rand_effects_shapiro) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T) 
```

* Assumption 3: Appropriate estimation of variance (no overdispersion)
    * Overdispersion is not a problem here. The empirical variance in data does not exceed the nominal variance under the presumed model. The chi-square test of the ratio of the empirical variance in data and the nominal variance under the presumed model is not significant:
```{r test GLMM assumption 3}

overdisp_fun <- function(model) {
  rdf <- df.residual(model)
  rp <- residuals(model,type="pearson")
  Pearson.chisq <- sum(rp^2)
  prat <- Pearson.chisq/rdf
  pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)
  c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)
}
overdisp_fun(GLMM_acc_final) # If the p-value is < 0.05, the data are overdispersed. Here p > 0.05. Overdispersion is not a problem here. 
```

<br>
<br>
<br>

# Comparison with ANOVAs

<br>

#### ANOVA RT and Accuracy

I calculated an univariate two-way repeated measures ANOVAs, containing the within-subjects factors response type (levels FA, FH, SH, CI) and word valence (levels neg, pos). 
Aarts et al. (2012, 2013) only used 2 to 3 levels of the factor response type (FA, FH, SH). They agued that the word categorization was not influenced by preceding CIs and SHs and thus did not include these factor levels in the ANOVA. To my mind, it makes sense to include all levels (also due to the fact that we find an effect after SHs). 

For RT, I also computed an ANOVA with untransformed RTs. The results are qualitatively the same. There is an additional tend for a main effect of word valence (p = .054). Also the results of all post hoc comparisons remain the same (with pairwise *t* tests and with Tukey).

```{r word ANOVAs, fig.width=10, fig.height=6}
options(contrasts=c("contr.sum","contr.poly")) # to adhere to the sum-to-zero convention for effect weights, always do this before running ANOVAs in R. This matters sometimes (not always). If I do not do it, the sum of squares calculations may not match what I get e.g. in SPSS

# plot Acc and RT per subject
# ggplot(data=df_rt_aggregated, aes(x=condition, y = rt))+geom_point()+facet_wrap(~subjectID)+theme_bw()
# ggplot(data=df_acc_aggregated, aes(x=condition, y = percent_correct_responses))+geom_point()+facet_wrap(~subjectID)+theme_bw() 
 

# Calculate ANOVAs
# I used aov_ez because apa_print does not work for ezAnova and ezAnova does not give residuals but aov_ez does (important for checking normality of residuals)
# aov_ez gives same result as ezAnova except that aov_ez reports results directly corrected for shericity violation; ezANOVA gives same result as SPSS :)
# I used afex; see advantages here: https://www.r-bloggers.com/anova-in-r-afex-may-be-the-solution-you-are-looking-for/
anova_rt <- aov_ez(id = "subjectID", dv = "rt_inverse", data = df_aggregated_per_subject, within = c("response_type","word_valence"))
apa_anova_rt <- apa_print(anova_rt)
# summary(anova_rt) to see results of Mauchly's Sphericity Test -> sign. -> sphericity not given
# alternative with ezANOVA: anova_rt <- ezANOVA(data = df_aggregated_per_subject,dv = .(rt_inverse), wid = .(subjectID),within = .(response_type, word_valence),detailed = TRUE)

anova_accuracy <- aov_ez(id = "subjectID", dv = "percent_correct_responses", data = df_aggregated_per_subject, within = c("response_type","word_valence"))
apa_anova_accuracy <- apa_print(anova_accuracy)
# summary(anova_accuracy) to see results of Mauchly's Sphericity Test -> sign. -> sphericity not given


kable(apa_anova_accuracy$table, caption = "Accuracy Word Categorization")  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_right")

kable(apa_anova_rt$table, caption = "Reaction Time Word Categorization")  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")

# output: mean square error (MSE) and generalized 2 (ges) as an effect size measure


# plot ANOVA
plot_anova_rt       <- afex_plot(anova_rt, x = "response_type", trace = "word_valence", error = "within", mapping = c("color", "fill")) + theme_light()
plot_anova_accuracy <- afex_plot(anova_accuracy, x = "response_type", trace = "word_valence", error = "within", mapping = c("color", "fill")) + theme_light()

# save legend
legend <- get_legend(plot_anova_rt) 

# remove legends from plots
plot_anova_rt       <- plot_anova_rt + theme(legend.position="none") 
plot_anova_accuracy <- plot_anova_accuracy + theme(legend.position="none") 

grid.arrange(plot_anova_rt,plot_anova_accuracy,legend, nrow=1,widths=c(2.5, 2.5, 0.6))


# Comparison with logistic regression
# summary(regRT <-  lm(word_rt_inverse~ response_type*valence, data = data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]))
# AIC(regRT) # -167924.9 cf. LMM AIC: -174628.7
# BIC(regRT) # -167857.6 cf. LMM BIC: -174523.9

# summary(reg_acc <- glm(word_accuracy~ response_type*valence,family=binomial(link='logit'), data=data4mixedmodels_words))# AIC 7065.1
# BIC(reg_acc) # 7125.571 -> GLMM is smaller
# summary(reg_acc_nested <- glm(word_accuracy~ response_type/valence,family=binomial(link='logit'), data=data4mixedmodels_words))
```
<br>
<br>
<br>
<br>


#### Post Hoc Tests

For within-subject repeated measures ANOVAs, pairwise *t* tests with an adjustment for multiple comparisons (e.g. Bonferroni or Holm) are recommended (see Evernote entry, 2019_07_26). The violation of sphericity in within subjects designs makes the error term incorrect and the Tukey approach problematic. For my RTs and the interaction effect of accuracy, Tukey post hoc tests come to the same result anyway. Pairwise t tests with Holm adjustments are presented here.


**Main Effect of Response Type on RT:**
RT after FA = CI and after FH = SH, but FA, CI > FH, SH. However, due to the presence of a disordinal interaction effect, this main effect cannot be sensibly interpreted. Hence, results for F tests of the main effect will be reported, but interpretation and discussion will be limited to the significant interaction effect.


**Interaction Reyponse Type x Word Valence on RT:**
RT for negative and positive words differs significantly after FA, FH, and SH but not after CI.


**Main Effect of Response Type on Accuracy:**
According to pairwise comparison with *t* tests, all conditions differ, only FH = SH. With regard to the visualized results, this is a bit strange (FH/SH and CI seem quite similar). Using the Tukey post hoc adjustment, only FA differs from all other conditions (FH, SH, CI). But as before, due to the presence of a disordinal interaction effect, this main effect cannot be sensibly interpreted.

**Interaction Reyponse Type x Word Valence on RT:**
Accuracy for negative and positive words differs significantly after FA, but not after FH, SH, and CI.


<br>

```{r word ANOVAs post hoc tests}

### Get t values and df for post hoc tests
df_aggregated_per_subject_and_response_type <- df_aggregated_per_subject %>%
   group_by(subjectID, response_type) %>%
   summarise(rt_inverse = mean(rt_inverse), percent_correct_responses = mean(percent_correct_responses))

source("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses/functions/pairwise.t.test.with.t.and.df.R")
result_rt <- pairwise.t.test.with.t.and.df(df_aggregated_per_subject_and_response_type$rt_inverse,df_aggregated_per_subject_and_response_type$response_type, p.adjust.method="holm",paired=TRUE)

result_acc <- pairwise.t.test.with.t.and.df(df_aggregated_per_subject_and_response_type$percent_correct_responses,df_aggregated_per_subject_and_response_type$response_type, p.adjust.method="holm",paired=TRUE)


t_values_rt     <- round(result_rt[[5]],digits = 2)
dfs_rt          <- round(result_rt[[6]],digits = 2)
t_values_acc    <- round(result_acc[[5]],digits = 2)
dfs_acc         <- round(result_acc[[6]],digits = 2)

### Get p values for post hoc tests
anova_rt_posthoc_response_type       <- pairwise.t.test(df_aggregated_per_subject_and_response_type$rt_inverse, df_aggregated_per_subject_and_response_type$response_type, p.adjust.method="holm", paired=TRUE)

anova_accuarcy_posthoc_response_type <- pairwise.t.test(df_aggregated_per_subject_and_response_type$percent_correct_responses, df_aggregated_per_subject_and_response_type$response_type, p.adjust.method="holm", paired=TRUE)



pairwise_t_tests_rt_interaction <- data.frame(c(t.test(df_wide$rt_inverse.neg_after_SH,df_wide$rt_inverse.pos_after_SH, paired = TRUE)$statistic,        
                                                t.test(df_wide$rt_inverse.neg_after_FH,df_wide$rt_inverse.pos_after_FH, paired = TRUE)$statistic,
                                                t.test(df_wide$rt_inverse.neg_after_FA,df_wide$rt_inverse.pos_after_FA, paired = TRUE)$statistic,
                                                t.test(df_wide$rt_inverse.neg_after_CI,df_wide$rt_inverse.pos_after_CI, paired = TRUE)$statistic),
                                              c(t.test(df_wide$rt_inverse.neg_after_SH,df_wide$rt_inverse.pos_after_SH, paired = TRUE)$parameter,
                                                t.test(df_wide$rt_inverse.neg_after_FH,df_wide$rt_inverse.pos_after_FH, paired = TRUE)$parameter,
                                                t.test(df_wide$rt_inverse.neg_after_FA,df_wide$rt_inverse.pos_after_FA, paired = TRUE)$parameter,
                                                t.test(df_wide$rt_inverse.neg_after_CI,df_wide$rt_inverse.pos_after_CI, paired = TRUE)$parameter),
                                              c(t.test(df_wide$rt_inverse.neg_after_SH,df_wide$rt_inverse.pos_after_SH, paired = TRUE)$p.value,
                                                t.test(df_wide$rt_inverse.neg_after_FH,df_wide$rt_inverse.pos_after_FH, paired = TRUE)$p.value,
                                                t.test(df_wide$rt_inverse.neg_after_FA,df_wide$rt_inverse.pos_after_FA, paired = TRUE)$p.value,
                                                t.test(df_wide$rt_inverse.neg_after_CI,df_wide$rt_inverse.pos_after_CI, paired = TRUE)$p.value),
                                         row.names = c("SH neg vs pos","FH neg vs pos","FA neg vs pos","CI neg vs pos"))
colnames(pairwise_t_tests_rt_interaction) <- c("t_value","df","raw_p_value")
pairwise_t_tests_rt_interaction[,c("t_value","raw_p_value")] <- c(round(pairwise_t_tests_rt_interaction[,"t_value"],digits=2), round(pairwise_t_tests_rt_interaction[,"raw_p_value"],digits=3))
pairwise_t_tests_rt_interaction <- pairwise_t_tests_rt_interaction[order(pairwise_t_tests_rt_interaction$raw_p_value),]  # order acc to p values for holm correction
pairwise_t_tests_rt_interaction$holm_p_value <- p.adjust(pairwise_t_tests_rt_interaction$raw_p_value, method = "holm")
pairwise_t_tests_rt_interaction <- pairwise_t_tests_rt_interaction[c("SH neg vs pos","FH neg vs pos","FA neg vs pos","CI neg vs pos"),]


pairwise_t_tests_acc_interaction <- data.frame(c(t.test(df_wide$percent_correct_responses.neg_after_SH, 
                                                        df_wide$percent_correct_responses.pos_after_SH, paired = TRUE)$statistic,  
                                                 t.test(df_wide$percent_correct_responses.neg_after_FH, 
                                                        df_wide$percent_correct_responses.pos_after_FH, paired = TRUE)$statistic,
                                                 t.test(df_wide$percent_correct_responses.neg_after_FA, 
                                                        df_wide$percent_correct_responses.pos_after_FA, paired = TRUE)$statistic,
                                                 t.test(df_wide$percent_correct_responses.neg_after_CI, 
                                                        df_wide$percent_correct_responses.pos_after_CI, paired = TRUE)$statistic),
                                               c(t.test(df_wide$percent_correct_responses.neg_after_SH, 
                                                        df_wide$percent_correct_responses.pos_after_SH, paired = TRUE)$parameter,
                                                 t.test(df_wide$percent_correct_responses.neg_after_FH, 
                                                        df_wide$percent_correct_responses.pos_after_FH, paired = TRUE)$parameter,
                                                 t.test(df_wide$percent_correct_responses.neg_after_FA, 
                                                        df_wide$percent_correct_responses.pos_after_FA, paired = TRUE)$parameter,
                                                 t.test(df_wide$percent_correct_responses.neg_after_CI, 
                                                        df_wide$percent_correct_responses.pos_after_CI, paired = TRUE)$parameter),
                                               c(t.test(df_wide$percent_correct_responses.neg_after_SH, 
                                                        df_wide$percent_correct_responses.pos_after_SH, paired = TRUE)$p.value,
                                                 t.test(df_wide$percent_correct_responses.neg_after_FH, 
                                                        df_wide$percent_correct_responses.pos_after_FH, paired = TRUE)$p.value,
                                                 t.test(df_wide$percent_correct_responses.neg_after_FA, 
                                                        df_wide$percent_correct_responses.pos_after_FA, paired = TRUE)$p.value,
                                                 t.test(df_wide$percent_correct_responses.neg_after_CI, 
                                                        df_wide$percent_correct_responses.pos_after_CI, paired = TRUE)$p.value),
                                         row.names = c("SH neg vs pos","FH neg vs pos","FA neg vs pos","CI neg vs pos"))
colnames(pairwise_t_tests_acc_interaction) <- c("t_value","df","raw_p_value")
pairwise_t_tests_acc_interaction[,c("t_value","raw_p_value")] <- c(round(pairwise_t_tests_acc_interaction[,"t_value"],digits=2), round(pairwise_t_tests_acc_interaction[,"raw_p_value"],digits=3))
pairwise_t_tests_acc_interaction <- pairwise_t_tests_acc_interaction[order(pairwise_t_tests_acc_interaction$raw_p_value),]  # order acc to p values for holm correction
pairwise_t_tests_acc_interaction$holm_p_value <- p.adjust(pairwise_t_tests_acc_interaction$raw_p_value)
pairwise_t_tests_acc_interaction <- pairwise_t_tests_acc_interaction[c("SH neg vs pos","FH neg vs pos","FA neg vs pos","CI neg vs pos"),]





### Get effect size for post hoc tests
library("rstatix")
eff_sizes_Resp_type_RT <- df_aggregated_per_subject_and_response_type[,-1]  %>% cohens_d(rt_inverse ~ response_type, paired = TRUE)
eff_sizes_Resp_type_RT$effsize <- round(eff_sizes_Resp_type_RT$effsize,digits = 2)

eff_sizes_interaction_RT <- df_aggregated_per_subject  %>% cohens_d(rt_inverse ~ condition, paired = TRUE,comparisons = list(c("neg_after_SH", "pos_after_SH"), c("neg_after_FH", "pos_after_FH"),c("neg_after_FA", "pos_after_FA"), c("neg_after_CI", "pos_after_CI")))
eff_sizes_interaction_RT$effsize <- round(eff_sizes_interaction_RT$effsize,digits = 2)

eff_sizes_Resp_type_acc <- df_aggregated_per_subject_and_response_type[,-1] %>% cohens_d(percent_correct_responses ~ response_type, paired = TRUE)
eff_sizes_Resp_type_acc$effsize <- round(eff_sizes_Resp_type_acc$effsize,digits = 2)

eff_sizes_interaction_acc <- df_aggregated_per_subject  %>% cohens_d(percent_correct_responses ~ condition, paired = TRUE,comparisons = list(c("neg_after_SH", "pos_after_SH"), c("neg_after_FH", "pos_after_FH"),c("neg_after_FA", "pos_after_FA"), c("neg_after_CI", "pos_after_CI")))
eff_sizes_interaction_acc$effsize <- round(eff_sizes_interaction_acc$effsize,digits = 2)



kable(cbind(t_values_rt,dfs_rt), caption = "RT: t-values and dfs")   %>%
 add_header_above(c(" ", "t-values" = 3, "dfs" = 3)) %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")

kable(round(anova_rt_posthoc_response_type$p.value,digits=3), caption = "RT: p values")  %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")  %>%
 footnote(general = "P value adjusted with Holm method.")

kable(eff_sizes_Resp_type_RT[,c(2:7)], caption = "RT: effect sizes")  %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")


kable(cbind(t_values_acc,dfs_acc), caption = "Acc: t-values and dfs")   %>%
 add_header_above(c(" ", "t-values" = 3, "dfs" = 3)) %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")

kable(round(anova_accuarcy_posthoc_response_type$p.value,digits=3), caption = "Accuracy: p values")  %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")  %>%
 footnote(general = "P value adjusted with Holm method.")

kable(eff_sizes_Resp_type_acc[,c(2:7)], caption = "Acc: effect sizes")  %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")


kable(pairwise_t_tests_rt_interaction, caption = "RT Interaction: t-values, dfs, p-values")   %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left") 
   
kable(eff_sizes_interaction_RT[,c(2:7)], caption = "RT Interaction: effect sizes")  %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_right")

kable(pairwise_t_tests_acc_interaction, caption = "Acc Interaction: t-values, dfs, p-values")  %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left") 

kable(eff_sizes_interaction_acc[,c(2:7)], caption = "Acc Interaction: effect sizes")  %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_right")


# # using emmeans and tukey adjustment for multiple comparisons (default p-value adjustment is "tukey")
# afex_options(emmeans_model = "multivariate") # A caveat regarding the use of emmeans concerns the assumption of sphericity for ANOVAs including within-subjects/repeated-measures factors. The current default for follow-up tests uses a univariate model, which does not adequately control for violations of sphericity. This may result in anti-conservative tests and contrasts somewhat with the default ANOVA table which reports results based on the Greenhousse-Geisser correction. An alternative is to use a multivariate model  which should handle violations of sphericity better.
# 
# anova_rt_posthoc_response_type       <- emmeans(anova_rt, ~ response_type) # NOTE: Results may be misleading due to involvement in interactions 
# anova_rt_posthoc_response_type       <- pairs(anova_rt_posthoc_response_type)
# anova_rt_posthoc_interaction         <- emmeans(anova_rt, "word_valence", by = "response_type")
# anova_rt_posthoc_interaction         <- pairs(anova_rt_posthoc_interaction)
# 
# anova_accuracy_posthoc_response_type <- emmeans(anova_accuracy, ~ response_type) # NOTE: Results may be misleading due to involvement in interactions 
# anova_accuracy_posthoc_response_type <- pairs(anova_accuracy_posthoc_response_type)
# anova_accuracy_posthoc_interaction   <- emmeans(anova_accuracy, "word_valence", by = "response_type")
# anova_accuracy_posthoc_interaction   <- pairs(anova_accuracy_posthoc_interaction)
# 
# 
# kable(anova_rt_posthoc_response_type, caption = "RT: Post Hoc Test Response Type")  %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")  %>%
#   footnote(general = "P value adjusted with Tukey method.")
# 
# kable(anova_accuracy_posthoc_response_type, caption = "Accuracy: Post Hoc Test Response Type")  %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_right")  %>%
#   footnote(general = "P value adjusted with Tukey method.")
# 
# kable(anova_rt_posthoc_interaction, caption = "RT: Post Hoc Test Response Type x Word Valence")  %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")  %>%
#   footnote(general = "P value adjusted with Tukey method.")
# 
# kable(anova_accuracy_posthoc_interaction, caption = "Accuracy: Post Hoc Test Response Type x Valence")  %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_right")  %>%
#   footnote(general = "P value adjusted with Tukey method.")
```



<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

#### Testing Normality of Residuals

Residuals of the transformed RT are normally distributed in each condition. Residuals of accuracy data are not normally distributed in any condition.

```{r test normality residuals, fig.width=10, fig.height=8}

residuals_rt                         <- data.frame(residuals(anova_rt$lm)) 
residuals_rt_shapiro                 <- do.call(rbind, lapply(residuals_rt[,], function(x) shapiro.test(x)["p.value"]))
residuals_rt_shapiro                 <- unlist(residuals_rt_shapiro[,1])
residuals_rt_shapiro                 <- data.frame(round(residuals_rt_shapiro, digits = 2))
residuals_rt_shapiro$condition       <- rownames(residuals_rt_shapiro)

residuals_accuracy                   <- data.frame(residuals(anova_accuracy$lm)) 
residuals_accuracy_shapiro           <- do.call(rbind, lapply(residuals_accuracy[,], function(x) shapiro.test(x)["p.value"]))
residuals_accuracy_shapiro           <- unlist(residuals_accuracy_shapiro[,1])
residuals_accuracy_shapiro           <- data.frame(round(residuals_accuracy_shapiro, digits = 2))
residuals_accuracy_shapiro$condition <- rownames(residuals_accuracy_shapiro)

residuals_shapiro           <- merge(residuals_rt_shapiro, residuals_accuracy_shapiro, by = "condition")
residuals_shapiro           <- cbind(residuals_shapiro$condition, round(residuals_shapiro[,2:3], digits = 3))
colnames(residuals_shapiro) <- c("Condition","Shapiro_Wilk_p_value_RT","Shapiro_Wilk_p_value_Accuracy")
residuals_shapiro$Shapiro_Wilk_p_value_RT[residuals_shapiro$Shapiro_Wilk_p_value_RT < 0.001]             <- "< 0.001"
residuals_shapiro$Shapiro_Wilk_p_value_Accuracy[residuals_shapiro$Shapiro_Wilk_p_value_Accuracy < 0.001] <- "< 0.001"

kable(residuals_shapiro, caption = 'Normality Residuals of Transformed RT and Accuracy') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Normality Residuals of Transformed RT" = 1, "Normality Residuals of Accuracy" = 1))



hist_residuals_word_rt_inverse <- ggplot(gather(residuals_rt, cols, value), aes(x = value)) + 
       geom_histogram(color="gray33", fill = "lightcyan3", binwidth = 0.1) + facet_grid(.~cols) +
       labs (title = "Histogram Residuals Word RT Inverse", x = "Residual Word RT Inverse", y ="Count") + 
       theme(plot.title = element_text(hjust = 0.5)) 

qqplot_residuals_word_rt_inverse <- ggplot(gather(residuals_rt, cols, value), aes(sample = value)) + 
  stat_qq(color = "lightcyan3") +
  facet_grid(.~cols) +
  labs (title = "Q-Q-Plot Residuals Word RT Inverse", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))

hist_residuals_word_accuracy <- ggplot(gather(residuals_accuracy, cols, value), aes(x = value)) + 
       geom_histogram(color="gray33", fill = "lightgoldenrod", binwidth = 7) + facet_grid(.~cols) +
       labs (title = "Histogram Residuals Accuracy", x = "Residual Word Accuracy", y ="Count") + 
       theme(plot.title = element_text(hjust = 0.5)) 

qqplot_residuals_word_accuracy <- ggplot(gather(residuals_accuracy, cols, value), aes(sample = value)) + 
  stat_qq(color = "lightgoldenrod") +
  facet_grid(.~cols) +
  labs (title = "Q-Q-Plot Residuals Word Accuracy", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(hist_residuals_word_rt_inverse,qqplot_residuals_word_rt_inverse,hist_residuals_word_accuracy,qqplot_residuals_word_accuracy,nrow = 4)
```

<br>

#### Evaluating Assumptions ANOVA

* **Assumption #1:** Dependent variable interval or ratio variable 

* **Assumption #2:** Balanced design (each subject has to have a value in each condition) 

* **Assumption #3:** No dependency in the scores between participants (dependency can exist only across scores for individuals) 

* **Assumption #4:** Distribution of residuals of the dependent variable in each level of the within-subjects factor is approximately normally distributed 

* **Assumption #5:** Sphericity  (variance of difference scores computed between any two levels of a within subject factor must be equal) (Mauchly's Sphericity Test and Greenhouse Geisser Correction applied to dfs)

<br>

* **Assumptions 1, 2, 3 are fulfilled for RT and Accuracy**

* **Assumption 4 (normal distribution of residuals) is fulfilled for RT and violated for accuracy**

* **Assumption 5 (sphericity) is violated but corrected for RT and accuracy**


Normality is not given for the aggregated accuracy data (percent correct in each condition). Non-parametric alternatives for repeated measures designs (such as the Friedman test) are designed only for **one** between- / within-subject factor. Normalization of the data by transformation does not work. The *boxcox* function suggested no plausible transformation to normalize these data. After square transformation the data (and residuals) are still heavily skewed. Using log or cube transformation does not work either. So I can only run the ANOVA with the non-normal aggregated accuracy data. For accuracy on single-trial level, I computed a GLMM. The results are consistent with the ANOVA, so everything is fine. 


<br>
<br>
<br>
<br>

# Plots

<br>

#### Bar Plots

```{r bar plots, fig.width=10}

df_aggregated_over_subjects_rt$response_type  <- factor(df_aggregated_over_subjects_rt$response_type, levels=c("SH","FH","FA","CI"))
df_aggregated_over_subjects_acc$response_type <- factor(df_aggregated_over_subjects_acc$response_type, levels=c("SH","FH","FA","CI"))


barplot_rt <- ggplot(df_aggregated_over_subjects_rt,aes (x = response_type,y = word_rt, fill = word_valence)) +
  geom_bar(stat="identity", position=position_dodge()) +                                                       
  geom_errorbar(aes(ymax = word_rt + ci, ymin = word_rt - ci), position = position_dodge(width=0.95), width=0.1) +    
 # ggtitle("RT Word Categorization") +                                                                            
  xlab("Preceding Response Type") + ylab("Response Time (ms)") +                                                                  
  guides(fill=guide_legend(title="Word Valence")) +                                                                         
  theme(axis.title = element_text(size = 18, hjust = 0.5)) +                                                   # center title
  theme(axis.text=element_text(size=15)) +
  theme(legend.title=element_text(size=15)) +
  theme(legend.text=element_text(size=15)) +
  theme(plot.title = element_text(size = 20, hjust = 0.5)) +
  theme(legend.position="bottom") +
  coord_cartesian(ylim = c(300,900)) +
  scale_fill_manual(values=c("steelblue4", "slategray1")) +
  geom_signif(y_position=c(850,850,850,850), xmin=c(0.75, 1.75,2.75,3.75), xmax=c(1.25, 2.25,3.25,4.25),
              annotation=c("**","***","***", "NS"), tip_length=0.02) 
  # +ggsave("plot_rt.png", dpi=2000)

barplot_accuracy <- ggplot(df_aggregated_over_subjects_acc,aes (x = response_type,y = word_accuracy, fill = word_valence)) +
  geom_bar(stat="identity", position=position_dodge()) +                                                       
  geom_errorbar(aes(ymax = word_accuracy + ci, ymin = word_accuracy - ci),    position = position_dodge(width=0.95), width=0.1) +  
 # ggtitle("Accuracy Word Categorization") +                                                                    
  xlab("Preceding Response Type") + ylab("Hit Rate") +                                                           
  guides(fill=guide_legend(title="Word Valence")) +                                                            
  theme(axis.title = element_text(size = 18, hjust = 0.5)) +                                                   # center title
  theme(axis.text=element_text(size=15)) +
  theme(legend.title=element_text(size=15)) +
  theme(legend.text=element_text(size=15)) +
  theme(plot.title = element_text(size = 20, hjust = 0.5)) +
  theme(legend.position="bottom") +
  coord_cartesian(ylim = c(0.30,1.20)) +
  scale_fill_manual(values=c("steelblue4", "slategray1"))  +
  geom_signif(y_position=c(1.125,1.125,1.125,1.125), xmin=c(0.75, 1.75,2.75,3.75), xmax=c(1.25, 2.25,3.25,4.25),
              annotation=c("NA","NS","***", "NS"), tip_length=0.02)                                                      
  # +ggsave("plot_acc.png", dpi=2000)



# save legend
legend <- get_legend(barplot_rt) 

# remove legends from plots
barplot_rt <- barplot_rt + theme(legend.position="none") 
barplot_accuracy <- barplot_accuracy + theme(legend.position="none") 


grid.arrange(legend, nrow=2,arrangeGrob(barplot_rt,barplot_accuracy,nrow=1),heights=c(1, 10),top = textGrob('Behavioral Performance Word Categorization', gp = gpar(fontsize = 20)))

# save image
# priming <- grid.arrange(legend, nrow=2,arrangeGrob(barplot_rt,barplot_accuracy,nrow=1),heights=c(1, 10),top = textGrob('Behavioral Performance Word Categorization', gp = gpar(fontsize = 20)))
# grid.draw(priming) # interactive device
# ggsave("priming.png", priming, width = 36, height = 18, units = "cm")


### Further plot possibilities
# display as line plot
# barplot_rt <- ggplot(df_aggregated_over_subjects_rt,aes (x = response_type,y = word_rt, group = word_valence)) +
#   geom_point(size=3, aes(color = word_valence))  +     
#   geom_line(aes(color = word_valence)) +
#   geom_errorbar(aes(ymin=word_rt-ci, ymax=word_rt+ci,color = word_valence), width=0.1) +    
#   ggtitle("Response Time Word Categorization") +                                                                            
#   xlab("Preceding Response Type") + ylab("Response Time (ms)") +                                                                  
#   guides(fill=guide_legend(title="Word Valence")) +                                                                         
#   theme(axis.title = element_text(size = 18, hjust = 0.5)) +                                                   # center title
#   theme(axis.text=element_text(size=15)) +
#   theme(legend.title=element_text(size=15)) +
#   theme(legend.text=element_text(size=15)) +
#   theme(plot.title = element_text(size = 20, hjust = 0.5)) +
#   theme(legend.position="bottom") +
#   coord_cartesian(ylim = c(300,900)) +
#   scale_color_manual(values=c("steelblue4", "slategray1")) 


# plot data whith removing all random effects by using remef
# library(remef)
# data4mixedmodels_words$RT_DV <- NA
# data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,]$RT_DV <- remef(LMM_rt_final, ran="all") # remove all random effects
# tab_lmmse <- ddply(data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], "condition", summarize, 
#                    N       = length(RT_DV),
#                    word_rt = mean(RT_DV),
#                    sd      = sd(RT_DV),
#                    se      = sd/sqrt(N),
#                    ci      = 1.96*se)
# tab_lmmse$response_type <- factor(substr(tab_lmmse$condition,11,12), levels = c("FA","FH","SH","CI"))
# tab_lmmse$word_valence  <- factor(substr(tab_lmmse$condition,1,3))
# ggplot(tab_lmmse,aes (x = response_type,y = word_rt, fill = word_valence)) +
#   geom_bar(stat="identity", position=position_dodge()) +                                                       
#   geom_errorbar(aes(ymin=word_rt-se, ymax=word_rt+se), position = position_dodge(width=0.95), width=0.1) +
#   scale_y_reverse( lim=c(0,-2))
```
<br>

Illustration of behavioral performance (raw data) in the word categorization depending on word valence and preceding response type. Error bars depict 95% CI. Values are adjusted for within-participant designs following Morey (2008). Calculation of error bars was based on the non-aggregated data, as this better reflects the LMM approach. Computing error bars based on the aggregated data per subject yields different results, as in the mean of the group means it would not be considered that subjects have different number of events per condition; this more resembles the ANOVA approach, because it assumes that each subject mean is equally reliable (relies on the same amount of information), which is not true.

<br>

#### Priming in Each Subject

As we can see, the facilitation to respond faster and more correctly to a negative word after a FA and to a positive word after a FH is present in almost each subject. 

```{r boxplots, fig.height=6, fig.width=10}

# preparing data for boxplot (wide to long format)
df4primingplots <-   reshape(data = df_wide, 
                      direction = "long",
                      varying = list(c("rt_priming_after_FA","rt_priming_after_FH"),c("accuracy_priming_after_FA","accuracy_priming_after_FH")),
                      v.names = c("rt","accuracy"),
                      idvar = "subject",
                      timevar = "condition",
                      times = c("priming_after_FA","priming_after_FH"))
row.names(df4primingplots) <- NULL


boxplot_priming_rt <- ggplot(df4primingplots, aes(x = condition, y = rt, fill = condition)) + 
  geom_boxplot(outlier.shape = NA) +                # remove outliers here, otherwise they are plotted twice (also as data point)
  geom_point(aes(color = condition), colour = "white", size = 2, shape=21, position = position_jitter(0.2)) +
  ggtitle("Priming Effect Response Time Word Categorization") +                                                   
  ylab("RT Facilitation (ms)") +                                                                                  
  scale_x_discrete(labels=c("priming_after_FA" = "Pos - Neg after False Alarm", "priming_after_FH" = "Neg - Pos after Fast Hit")) +
  theme(axis.title = element_text(hjust = 0.5)) +                                                                 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.title.x = element_blank()) +
  theme(legend.position = "none") +
  scale_fill_manual(values=c("steelblue4", "slategray1")) +
  scale_color_manual(values=c("steelblue4", "slategray1")) 
  #+ ggsave("plot_priming_rt.png", dpi=2000)

boxplot_priming_accuracy <- ggplot(df4primingplots, aes(x = condition, y = accuracy, fill = condition)) + 
  geom_boxplot(outlier.shape = NA) +          
  geom_point(aes(color = condition), colour = "white", size = 2, shape=21, position = position_jitter(0.2)) +
  ggtitle("Priming Effect Accuracy Word Categorization") +                                                   
  ylab("Accuracy Facilitation (%)") +                                                                                  
  scale_x_discrete(labels=c("priming_after_FA" = "neg - pos after False Alarm", "priming_after_FH" = "pos - neg after Fast Hit")) +
  theme(axis.title = element_text(hjust = 0.5)) +                                                                 
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.title.x = element_blank()) +
  theme(legend.position = "none") +
  scale_fill_manual(values=c("steelblue4", "slategray1")) +
  scale_color_manual(values=c("steelblue4", "slategray1")) 
  #+ ggsave("plot_priming_accuracy.png", dpi=2000)

grid.arrange(boxplot_priming_rt,boxplot_priming_accuracy,nrow = 1)
```

<br>

#### Tables Mean and CI/SE

```{r descriptive}

# this shows the data calculated based on the non aggregated (single trial) data. This corresponds to the LMM approach and my plots.
table_rt <- df_aggregated_over_subjects_rt[,c("condition","N","word_rt","sd","se","ci")]
table_rt[,c("word_rt","sd","se","ci")] <- round(table_rt[,c("word_rt","sd","se","ci")], digits = 2)
table_rt <- t(table_rt)
rownames(table_rt) <- c("","N","M","SD","SE","95% CI")
table_rt <- table_rt[,c(2,6,3,7,4,8,1,5)] # reorder columns

kable(table_rt, caption = 'Reaction Time') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")

table_acc <- df_aggregated_over_subjects_acc[,c("condition","N","word_accuracy","sd","se","ci")]
table_acc[,c("word_accuracy","sd","se","ci")] <- round(table_acc[,c("word_accuracy","sd","se","ci")], digits = 2)
table_acc <- t(table_acc)
rownames(table_acc) <- c("","N","M","SD","SE","95% CI")
table_acc <- table_acc[,c(2,6,3,7,4,8,1,5)] # reorder columns

kable(table_acc, caption = 'Hit Rate') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")


# this would show the data calculated first by aggregating by subject and then over subjects. This corresponds to the ANOVA approach
# descriptive_statistics[,1] <- row.names(descriptive_statistics)
# colnames(descriptive_statistics)[1] <- "descriptive statistic"
# 
# descriptive_statistics <- descriptive_statistics %>% 
#   mutate_at(c("rt.neg_after_FA", "rt.pos_after_FA", "rt.neg_after_FH", "rt.pos_after_FH","rt.neg_after_SH", "rt.pos_after_SH", "rt.neg_after_CI", "rt.pos_after_CI", "rt_priming_after_FA", "rt_priming_after_FH"), list(~ round(.,0)))
# 
# descriptive_statistics <- descriptive_statistics %>% 
#   mutate_at(c("percent_correct_responses.neg_after_FA", "percent_correct_responses.pos_after_FA", "percent_correct_responses.neg_after_FH", "percent_correct_responses.pos_after_FH", "percent_correct_responses.neg_after_SH", "percent_correct_responses.pos_after_SH", "percent_correct_responses.neg_after_CI", "percent_correct_responses.pos_after_CI", "accuracy_priming_after_FA", "accuracy_priming_after_FH"), list(~ round(.,1)))
# 
# kable(descriptive_statistics[c(2:4,6),c("descriptive statistic", "rt.neg_after_FA", "rt.pos_after_FA", "rt.neg_after_FH", "rt.pos_after_FH", "rt.neg_after_SH", "rt.pos_after_SH", "rt.neg_after_CI", "rt.pos_after_CI", "rt_priming_after_FA", "rt_priming_after_FH")], caption = 'Reaction Time') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")
# 
# kable(descriptive_statistics[c(2:4,6),c("descriptive statistic", "percent_correct_responses.neg_after_FA", "percent_correct_responses.pos_after_FA", "percent_correct_responses.neg_after_FH", "percent_correct_responses.pos_after_FH", "percent_correct_responses.neg_after_SH", "percent_correct_responses.pos_after_SH", "percent_correct_responses.neg_after_CI", "percent_correct_responses.pos_after_CI", "accuracy_priming_after_FA", "accuracy_priming_after_FH")], caption = 'Accuracy') %>%
#   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")
```

Note: The values here are based on the non-aggregated single trial data. This corresponds to the LMM approach and my plots.

<br>

#### Estimated Marignal Means Plot/Tab

I obtained estimated marginal means (aka least square means or adjusted means) using the emmeans package. I can plot the back-transformed estimated marginal means for all conditions and the CIs/SEs. For inverse transformation, there is no convenient way for also back-transforming contrast estimates (and difference CIs) for the LMM table (log and logit transforms are exceptional cases in that there is a valid way to back-transform contrasts: https://github.com/rvlenth/emmeans/blob/master/R/contrast.R). Hence, I will report the transformed values. In the plot of the estimated marginal means, the CIs of significantly differing conditions overlap considerably. SE are smaller (CI = 1.96 * SE), but many journals require CI, so I plot CI.

Some information on CIs: Overlapping confidence intervals say nothing about statistical significance. The inverse — non-overlapping confidence intervals — implies statistical significance. If the 95% CI of the difference contains 0, then there is no difference between conditions. When using a repeated measures or a paired group design, do not compare the CIs of the group means because there will be no meaningful interpretation. Instead, use a CI around the mean difference. Only for independent sample means: p values can be estimated from the graphs of the confidence intervals. If the CIs overlap by about 1/4, half the average margin of error, then p ≈ .05. When they are just touching, then p ≈ .01.  

In the plot I changed the abbreviation from CI (correct inhibition) to IR (inhibited response), because I need the abbreviation CI for confidence intervals in the manuscript.

```{r estimated mean plots, fig.width=10, fig.height=6}

# RT
# # I saved computed estimated marginal means because the computation takes very long
# emm_options(pbkrtest.limit = 13092)
# emm_options(lmerTest.limit = 13092)
# 
# # If I want to back transforn the inverse transformation, I can only do 1/RT
# tran <- make.tran("boxcox",-1)
# LMM4plot <- with(tran,
#                  lmer(linkfun(word_rt) ~ response_type*valence +(1 + FH_minus_SH + FA_minus_FH + CI_minus_FA + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + CI_minus_FA:pos_minus_neg || subjectID) + (1 + FH_minus_SH + FA_minus_FH || word), data=data4mixedmodels_words[data4mixedmodels_words$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa")))
# 
# start_time = Sys.time()
# Estimated_Means_Priming_RT = as.data.frame(emmeans(LMM4plot, ~ response_type*valence, type = "response"))
# Estimated_Means_Priming_Acc = as.data.frame(emmeans(GLMM_acc_final, ~ response_type*valence, type = "response"))
# end_time = Sys.time()
# end_time - start_time # Time difference of 39 mins
# Estimated_Means_Priming_RT$response_type <- factor(Estimated_Means_Priming_RT$response_type, levels = c("FA","FH","SH","CI"))
# 
# setwd("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses")
# save(Estimated_Means_Priming_RT,file="Estimated_Means_Priming_RT.rda")
# save(Estimated_Means_Priming_Acc,file="Estimated_Means_Priming_Acc.rda")
# 
# # # get back-transformed estimated marginal means (long computation time)
# # emmeans(LMM4plot, pairwise ~ response_type, type = "response")
# # emmeans(LMM4plot, pairwise ~ valence, type = "response")
# # emmeans(LMM4plot, pairwise ~ response_type*valence, type = "response")
# # 
# # # alternative plots of estimated marginal means
# # plot_est_means2 <- emmeans(LMM4plot, ~ response_type/valence, type = "response")
# # plot_est_means2 <- plot(plot_est_means2, by = "response_type")
# # plot_est_means2
# # 
# # plot_est_means3 <- emmip(LMM4plot, ~ valence | response_type, type = "response", CIs = TRUE) 
# # plot_est_means3

# RT
load(file = "Estimated_Means_Priming_RT.rda")
emm2$response_type <- revalue(emm2$response_type, c("CI"="IR")) # change level CI to IR
emm2$response_type <- factor(emm2$response_type, levels=c("SH","FH","FA","IR"))
emm2$valence       <- factor(emm2$valence, levels=c("pos","neg"))

plot_est_means_rt <- ggplot(emm2,aes (x = response_type,y = response, fill = valence)) +
  geom_bar(stat="identity", position=position_dodge(),colour="black", size=0.25) +
  geom_errorbar(aes(ymax = upper.CL, ymin = lower.CL), position = position_dodge(width=0.9), width=0.2,size=0.3) +    
  theme_apa(base_size = 11) + # from papaja package 
  coord_cartesian(ylim = c(400,800)) +
  scale_y_continuous(expand = c(0, 0)) +        # neat axes ends
  labs(x="Preceding Response Type", y="Predicted RT (ms)") +
# scale_fill_grey(start = 0.60, end = 0.4, name = "Word Valence", labels = c("Pos", "Neg")) +     # greyscales
  scale_fill_manual(values=c("#8ea6b4", "#465369"), name = "Word Valence", labels = c("Pos", "Neg")) +
  theme(legend.position = 'top', axis.ticks.x=element_blank(), axis.title.y = element_text(vjust=-0.5)) +       # remove tick marks on x axis and bring y axis title closer to the plot
  geom_signif(y_position=c(785,785,785), xmin=c(0.75, 1.75,2.75), xmax=c(1.25, 2.25,3.25),
              annotation=c("**","***","***"), tip_length=0.01, vjust = 0.5, textsize = 4, size = 0.3)  

# Accuracy
load(file = "Estimated_Means_Priming_Acc.rda")
Estimated_Means_Priming_Acc$response_type <- revalue(Estimated_Means_Priming_Acc$response_type, c("CI"="IR")) # change level CI to IR
Estimated_Means_Priming_Acc$response_type <- factor(Estimated_Means_Priming_Acc$response_type, levels = c("SH","FH","FA","IR"))
Estimated_Means_Priming_Acc$valence       <- factor(Estimated_Means_Priming_Acc$valence, levels=c("pos","neg"))
Estimated_Means_Priming_Acc[,c(3,4,6,7)]  <- Estimated_Means_Priming_Acc[,c(3,4,6,7)]*100   # calculate probability in %

plot_est_means_acc <- ggplot(Estimated_Means_Priming_Acc,aes (x = response_type,y = prob, fill = valence)) +
  geom_bar(stat="identity", position=position_dodge(),colour="black", size = 0.25) +                                         
  geom_errorbar(aes(ymax = asymp.UCL, ymin = asymp.LCL), position = position_dodge(width=0.9), width=0.2,size=0.3) +      
  theme_apa(base_size = 11) + # from papaja package 
  coord_cartesian(ylim = c(50,100)) +
  scale_y_continuous(expand = c(0, 0)) +        # neat axes ends
  labs(x="Preceding Response Type", y="Predicted Accuracy (%)") +
# scale_fill_grey(start = 0.60, end = 0.4) +    # greyscales
  scale_fill_manual(values=c("#8ea6b4", "#465369"), name = "Word Valence", labels = c("Pos", "Neg")) +
  theme(axis.ticks.x=element_blank(), axis.title.y = element_text(vjust=-0.5)) +       # remove tick marks on x axis and bring y axis title closer to the plot
  geom_signif(y_position=c(98.2), xmin=c(2.775), xmax=c(3.225),
              annotation=c("***"), tip_length=0.01, vjust = 0.5, textsize = 4, size = 0.3)  

                                               
  # save legend
  legend <- get_legend(plot_est_means_rt) 
  
  # remove legends from plots
  plot_est_means_rt  <- plot_est_means_rt + theme(legend.position="none") 
  plot_est_means_acc <- plot_est_means_acc + theme(legend.position="none") 
  
  # alternative without cowplot package  but also withous figure labels "A" and "B"
  # grid.arrange(arrangeGrob(plot_est_means_rt,plot_est_means_acc,nrow=1),nrow=2,legend, heights=c(10, 2))
 
library("cowplot")
figure_priming_behav <- ggdraw() +
  draw_plot(plot_est_means_rt, x =  0, y = .2, width =.5, height = 0.7) +
  draw_plot(plot_est_means_acc,x = .5, y = .2, width =.5, height = 0.7) +
  draw_plot(legend, x =  0.25, y = 0.1, width =.5, height = 0) +
  draw_plot_label(c("A", "B"), c(0, 0.5), c(1, 1), size = 15)

figure_priming_behav
ggsave("figure_priming_behav.tiff",width = 16, height = 10 , units = "cm", dpi=600)


  kable(cbind(emm2[c(1,5,2,6,3,7,4,8),c(1:2)],round(emm2[c(1,5,2,6,3,7,4,8),c(3:7)], digits = 2)), col.names = c("response type", "valence", "Mean", "SE", "df", "lower CI", "upper CI"), caption = 'Estimated Marginal Means RT') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")
  
    kable(cbind(Estimated_Means_Priming_Acc[c(1,5,2,6,3,7,4,8),c(1:2)],round(Estimated_Means_Priming_Acc[c(1,5,2,6,3,7,4,8),c(3,4,5,6,7)], digits = 2)), col.names = c("response type", "valence", "Mean", "SE", "df", "lower CI", "upper CI"), caption = 'Estimated Marginal Means Acc') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")
```
<br>
<br>
<br>

# Summary

Following False Alarms, subjects respond faster to a negative than a positive word. They are also more accurate in responding to a negative word. Follwing Hits (independent of whether the Hit was Fast or Slow), subjects respond faster to a positive than a negative word.
