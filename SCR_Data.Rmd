---
title: "SCR Data"
author: "Luisa Balzus"
date: "31 Juli 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  tidy = TRUE,
  message = FALSE,
  warning = FALSE,
  dpi = 150
)
```

Detailed information on the preprocessing and analysis decisions can be found in my Endnote entry 2019_08_27.

SC was measured with a direct current using Brain Vision Recorder (Brain Products GmbH, Gilching, Germany). SCRs were sampled at 1000 Hz. SCRs were collected using Ag-AgCl electrodes filled with electrolyte gel and applied to the middle phalanges of the fourth and fifth fingers of the non-dominant (i.e., left for all subjects) hand.

As Figner et al. (2011) recommend that the sampling rate should be no lower than 100-200 Hz (such high sampling rates are not imperative to veridically represent a relatively slow signal like skin conductance, but more complex analysis approaches and smoothing procedures can benefit from higher sampling rates), the data were downsampled to 100 Hz. I used a 1st order butterworth low-pass filter with 5Hz cutoff and smoothed the data with the gaussian method with a window width of 200 ms. Increasing the width of the smoothing window increases the sensitivity and produces more SCRs. Figner et al. (2011) used a simple moving average across 500 ms, Benedek & Kaernbach (2010) as well as many other studies used 200 ms as gaussian window.

The SC data was analyzed using Continuous Decomposition Analysis (CDA) in Ledalab (Benedek & Kaernbach, 2010). Benedek & Kaernbach (2010) clearly recommend the CDA over the DDA, as it is more robust and efficient (the DDA is sensitive to data quality, e.g. artifacts) and allows for a more straightforward scoring of phasic activity. Following the recommendations from Benedek & Kaernbach (2010), the ISCR results will be reported. The ISCR represents the unbiased cumulative phasic activity within a given response period. If response peaks are considered, we would need to define a minimum amplitude to be counted as a response to determine whether a response has occurred (responses below this threshold will be set to zero and are sometimes excluded from the analyses). As a threshold of 0.01 microsiemens is quite common in the literature, I would choose this value. I entered it in the matlab batch, but as I only use ISCR to quantify the SCR, the threshold is irrelevant, because the ISCR does not rely on the amplitude but only on the area under the curve. Other papers relying on the ISCR do not report any threshold. Hence, no SCRs have to be set to 0. The response window was 1 to 4 s after the response to the GNG stimulus, because such a window is commonly used (Boucsein et al., 2012) and is also compatible with the observed response range in my data (based on visual inspection of FA vs Hit condition). 

Unlike some other authors (e.g. Neiss et al., 2019, who also calculated LMMs), I did not analyze SCRs only from those trials in which a person showed a response. I did not exclude zero responses from my analyses, as these represent meaningful events as well. 
Participants lacking measurable SCR on > 75% of all trials were classified as non-responders. All participants were below this threshold so no participants were excluded.

Raw SCR amplitudes were square root transformed to reduce skewness so that the data are suitable for parametric statistical analysis. Subsequently, the data were z-standardized within each subject so that the SCR can be directly and meaningfully compared between individuals. Only for the SCR LMM I used the transformed but unstandardized SCR data because the random effects already account for the individual differences. If I use the z standardized values instead (these have a nice normal distribution, not like the values that were only square root transformed and have a distribution that is cut at 0), the results remain the same. The distribution of the residuals is much better then. But for the reason mentionned above, I report the unstandardized SCR LMM.

I did not include CI in the analyses, because there is no response-locked SCR in this condition and unlike in all other conditions, there is no motor response. (Previously I included the stimulus-locked SCR in the CI trials as it makes no sense to take response-locked SCR value here, because there is no response and the evaluation trigger is sent late). As the other response types are confounded by the motor response and CI is not, this may explain why the SCR in CI trials is lower than in the other trials. 
I only included FA, FH, SH trials that had no invalid gng rt or word rt (these trials were consistently excluded for all analyses), were followed by correct word classification and were not preceded or followed by an incorrect response or wrong key in gng or word classification. These criteria were also used for z standardizing the SCR within each subject and for the LMM on priming and scr.

```{r libraries, include = FALSE}
library(pastecs)                 # for descriptive stats
library(ggplot2)                 # for plots
library(knitr)                   # for nice tables in html
library(kableExtra)              # for nice tables in html
library(ez)                      # for my ANOVAs
library(papaja)                  # for visualizing Anova with papaja
library(afex)                    # for Anova that can be visualized in APA style with papaja and for ANOVA plot
library(gridExtra)               # for arranging plots in a grid
library(dplyr)                   # for mutating
library(emmeans)                 # for ANOVA follow-up tests
library(ggsignif)                # for adding significance bars in plots
library(MASS)                    # for setting LMM contrasts
library(mvnormtest)              # for mshapiro.test
library(plyr)                    # for ddply()
library(dplyr)                   # for mutating
library(tidyr)                   # to do plot for all columns of a df
library(e1071)                   # for functions skewness and kurtosis
library(car)                     # for qqp function
library("GeneralizedHyperbolic") # for fitting inverse gaussian distribution
library(sjPlot)                  # for tab_model
library(Rmisc)                   # for summarySEwithin


# clear environment
rm(list=ls())

# force R to not use exponential notation
options(scipen = 999)

# reading in datafile
datafiles <- list.files("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses", pattern = ".rda") 
for (datafile in datafiles){  
  # appending full path to filename is necessary to open files in Rmd
  filename <- paste0("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses/",datafile) 
  load(file = filename)}


# get cleaned single-trial data (here I exclude GNG but not word outliers) 
# if I change criteria here, I also have to change them in Single Trial Script for Scaling and counting events!!
# I can easily go back to scaling based on all trials; just change it in single trial script; the results are comparable, only the correlation between SCR and Priming after FA gets almost sign. then (p = 0.054)
data4mixedmodels_words_scr <- data4mixedmodels[data4mixedmodels$outlier_words == FALSE & data4mixedmodels$gng_resp <= 44 & data4mixedmodels$gng_invalid_rt == FALSE & data4mixedmodels$word_resp <= 54 & data4mixedmodels$followed_or_preceded_by_FA_or_wrong_key == FALSE & !is.na(data4mixedmodels$iscr_gng_resp),]
# 7139 (if incorrect words are excluded) of 15480 trials left
data4mixedmodels_words_scr$response_type <- factor(data4mixedmodels_words_scr$response_type, levels = c("SH","FH","FA","CI"))


# aggregating within subjects per condition (needed for ANOVAs)
df_aggregated_scr_condition <- ddply(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], 
                                     .(subjectID, condition), 
                                     summarise,
                                     iscr_gng_resp_sqrt_z_score = mean(iscr_gng_resp_sqrt_z_score, na.rm = TRUE),
                                     for_SCR_count = length(condition)) 

df_aggregated_scr_response_type <- ddply(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], 
                                         .(subjectID, response_type), 
                                         summarise,
                                         iscr_gng_resp_sqrt_z_score = mean(iscr_gng_resp_sqrt_z_score, na.rm = TRUE),
                                         event_counts_incl_zero_scr = length(subjectID))



# aggregating over subjects per condition and calculate SE/CI corrected for within design (only needed for barplots)
source("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses/functions/summarySEwithinO.R")

df_aggregated_over_subjects <- summarySEwithinO(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], measurevar="iscr_gng_resp_sqrt_z_score", withinvars="condition", idvar="subjectID", conf.interval=.95)
df_aggregated_over_subjects$response_type <- factor(substr(df_aggregated_over_subjects$condition, 11, 12), levels=c("SH","FH","FA"))
df_aggregated_over_subjects$word_valence  <- factor(substr(df_aggregated_over_subjects$condition, 1, 3))

df_aggregated_over_subjects_resptype <- summarySEwithinO(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], measurevar="iscr_gng_resp_sqrt_z_score", withinvars="response_type", idvar="subjectID", conf.interval=.95)
df_aggregated_over_subjects_resptype$response_type <- factor(df_aggregated_over_subjects_resptype$response_type, levels=c("SH","FH","FA"))




# long to wide format and calculate priming effect -> variables in df_wide_scr are identical to those in df4save! I just avoid using df4save for the subsequent analyses... 
df_wide_scr_condition <-   reshape(data = df_aggregated_scr_condition, 
                      direction = "wide",
                      v.names = c("iscr_gng_resp_sqrt_z_score","for_SCR_count"),
                      idvar = "subjectID",
                      timevar = "condition",
                      drop = c("subject","response_type","word_valence")) # variables to drop before reshaping

df_wide_scr_response_type <-   reshape(data = df_aggregated_scr_response_type, 
                      direction = "wide",
                      v.names = "iscr_gng_resp_sqrt_z_score",
                      idvar = "subjectID",
                      timevar = "response_type") # variables to drop before reshaping

df_wide_scr <- left_join(df_wide_scr_condition,df_wide_scr_response_type, by = "subjectID")



# get descriptive statistics
descriptive_statistics <- stat.desc(df_wide_scr,basic=F)

# add columns for log / square root transformed but not z standardized SCR for distribution check
data4mixedmodels_words_scr$iscr_gng_resp_log  <- log(data4mixedmodels_words_scr$iscr_gng_resp+1)
data4mixedmodels_words_scr$iscr_gng_resp_sqrt <- sqrt(data4mixedmodels_words_scr$iscr_gng_resp)

# save aggregated df
# setwd("P:/Luisa_Balzus/1_PhD_Project/6_ModERN_Behavioral_Study/5_Analyses")    
# date_time <- format(Sys.time(), "%Y-%m-%d_%H-%M-%S")
# filename_aggregated <- paste("Data_SCR_Aggregated_For_",nrow(df_wide),"_subjects_",date_time, ".rda", sep = "")
# save(df_wide_scr,file=filename_aggregated) 
```

<br> 
<br>
<br>

# Data Inspection

<br>

#### Visualize SCR per Subject

<br>

```{r plot SCR per subject, fig.width=10, fig.height=8}

per_subj_scr <- ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x=response_type, y=iscr_gng_resp_sqrt)) + geom_point(position="jitter") + ggtitle("SCR per Subject") + theme(plot.title = element_text(hjust = 0.5)) + ylab("SCR [square root transformed (µS)]") + facet_wrap( ~ subjectID, nrow=5)
per_subj_scr

per_subj_scr_aggregated <-  ggplot(data=df_aggregated_scr_response_type, aes(x=response_type, y = iscr_gng_resp_sqrt_z_score))+geom_point()+facet_wrap(~subjectID)+theme_bw()+ ggtitle("Aggregated SCR per Subject") + theme(plot.title = element_text(hjust = 0.5)) + ylab("SCR [square root and z transformed (µS)]")
per_subj_scr_aggregated
```

<br>

#### Distribution SCR Data

The data are highly skewed. Even the log transformed z standardized values deviate from normality. According to the *boxcox* function, the optimal lambda to transform the raw data to normality would be -3.7. A lambda of -4 corresponds to a transformation of x = y^-4, which here did not produce a normal distribution either (but rather some weird distribution...). Compared to log transformed data (would correspond to lambda = 0), the square root transformed data (would correspond to lambda = 0.5) better approach normal distribution. According to papers on SCR analysis, either log or square root transformation may be chosen for transforming SCR data (Dawson et al., 2007; Figner et al., 2011). I will choose square root transformation, as here the data approach normality (see also section Normality of Single Trial SCR Data). 

We have no real theoretical reasons why zero responses should be excluded, so we keep them. Neiss et al. (2009) (also used LMMs) excluded zero responses with the following explanation: *"Within many individuals, the distribution of SCR was essentially bimodal, with responses at zero (no response), and then a range of responses of varying  magnitudes. We thus analyzed SCRs from those pictures where a person showed a response, so  that we model the relation between SCR and subjective arousal rating for pictures to which individuals responded".* I checked in the SCR LMM and the Priming_SCR LMM whether the results change if the zero SCR responses are excluded (and the SCR data distribution is nicer). The zero responses were excluded and then the final models obtained by using the full data set were run again, they were not build again for the reduced dataset without zero responses. In the SCR LMM, the effect FA > FH remains but there is also a significant effect FH > SH (this is just a trend when not excluding zero SCR responses). 

In the Priming-SCR LMM the effects FA - FH, FA - FH x Pos - Neg, FA - FH x SCR remain. FH vs SH now becomes a trend (was not a trend before). The trend for a main effect of SCR vanishes (p from .06 to .12) and the effect FH_vs_SH:pos_minus_neg only becomes a trend (p from .04 to .08; possibly due to reduced power). For the nested model, the results do not change at all. So excluding zero responses only has minimal effects on the results and does not affect the results of interest. From this view, it is ok to not exclude them.

```{r inspect distribution, fig.width=10, fig.height=10}

# overlay normal density function curve to see how closely it fits normal distribution for normal curve: important to add aes(y = .. density..) to geom_histogram
hist_scr_raw = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x=iscr_gng_resp)) +
  geom_histogram(aes(y = ..density..),color="gray33", fill = "lightcoral", binwidth = 0.2, size = 1) +
  stat_function(fun=dnorm, args=list(mean=mean(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp,na.rm = TRUE), sd=sd(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp,na.rm = TRUE)), color="black", size = 0.5) +
  geom_vline(aes(xintercept=mean(iscr_gng_resp)), color="black", linetype="dashed", size= 1) +
  labs (title = "Histogram SCR Raw", x = "ISCR Raw GNG Response", y ="Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_scr_raw = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(sample=iscr_gng_resp)) +
  stat_qq(color = "lightcoral") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot ISCR Raw GNG Response", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))


hist_scr_log = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x=iscr_gng_resp_log)) +
  geom_histogram(aes(y = ..density..),color="gray33", fill = "lightcoral", binwidth = 0.05, size = 1) +
  stat_function(fun=dnorm, args=list(mean=mean(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_log,na.rm = TRUE), sd=sd(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_log,na.rm = TRUE)), color="black", size = 0.5) +
  geom_vline(aes(xintercept=mean(iscr_gng_resp_log)), color="black", linetype="dashed", size= 1) +
  labs (title = "Histogram SCR Log", x = "ISCR Log GNG Response", y ="Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_scr_log = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(sample=log(iscr_gng_resp+1))) +
  stat_qq(color = "lightcoral") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot ISCR Log GNG Response", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))


hist_scr_sqrt = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x=iscr_gng_resp_sqrt)) +
  geom_histogram(aes(y = ..density..),color="gray33", fill = "lightcoral", binwidth = 0.05,size = 1) +
  stat_function(fun=dnorm, args=list(mean=mean(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_sqrt,na.rm = TRUE), sd=sd(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_sqrt,na.rm = TRUE)), color="black", size = 0.5) +
  geom_vline(aes(xintercept=mean(iscr_gng_resp_sqrt)), color="black", linetype="dashed", size= 1) +
  labs (title = "Histogram SCR Square Root", x = "ISCR Square Root GNG Response", y ="Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_scr_sqrt = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(sample=iscr_gng_resp_sqrt)) +
  stat_qq(color = "lightcoral") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot ISCR Square Root GNG Response", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))


hist_scr_log_z = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x=iscr_gng_resp_log_z_score)) +
  geom_histogram(aes(y = ..density..),color="gray33", fill = "lightcoral", binwidth = 0.2, size = 1) +
  stat_function(fun=dnorm, args=list(mean=mean(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_log_z_score,na.rm = TRUE), sd=sd(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_log_z_score,na.rm = TRUE)), color="black", size = 0.5) +
  geom_vline(aes(xintercept=mean(iscr_gng_resp_log_z_score)), color="black", linetype="dashed", size= 1) +
  labs (title = "Histogram SCR Log Z Score", x = "ISCR Log Z Score GNG Response", y ="Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_scr_log_z = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(sample=iscr_gng_resp_log_z_score)) +
  stat_qq(color = "lightcoral") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot ISCR Log Z Score GNG Response", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))


hist_scr_sqrt_z = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x=iscr_gng_resp_sqrt_z_score)) +
  geom_histogram(aes(y = ..density..),color="gray33", fill = "lightcoral", binwidth = 0.2, size = 1) +
  stat_function(fun=dnorm, args=list(mean=mean(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_sqrt_z_score,na.rm = TRUE), sd=sd(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_sqrt_z_score,na.rm = TRUE)), color="black", size = 0.5) +
  geom_vline(aes(xintercept=mean(iscr_gng_resp_sqrt_z_score)), color="black", linetype="dashed", size= 1) +
  labs (title = "Histogram SCR Square Root Z Score", x = "ISCR Square Root Z Score GNG Response", y ="Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_scr_sqrt_z = ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(sample=iscr_gng_resp_sqrt_z_score)) +
  stat_qq(color = "lightcoral") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot ISCR Square Root Z Score GNG Response", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))



grid.arrange(hist_scr_raw,qqplot_scr_raw,
             hist_scr_log,qqplot_scr_log,
             hist_scr_sqrt,qqplot_scr_sqrt,
             hist_scr_log_z,qqplot_scr_log_z,
             hist_scr_sqrt_z,qqplot_scr_sqrt_z,nrow = 5)
```

<br>

#### Normality of Aggregated SCR Data

The Shapiro-Wilk test is used to test normality of the aggregated data. The aggregated square root transformed z-standardized SCR data are normally distributed in all conditions except for the FAs. The results are the same for the log transformed z-standardized SCR (not shown here) - it would also be ok to use log transformed z-standardized SCR data for the ANOVA.

```{r test distribution SCR aggregated, fig.width=10, fig.height=6}

normality <- do.call(rbind, lapply(df_wide_scr[,c("iscr_gng_resp_sqrt_z_score.SH","iscr_gng_resp_sqrt_z_score.FA","iscr_gng_resp_sqrt_z_score.FH","iscr_gng_resp_sqrt_z_score.neg_after_SH","iscr_gng_resp_sqrt_z_score.pos_after_SH","iscr_gng_resp_sqrt_z_score.neg_after_FA","iscr_gng_resp_sqrt_z_score.pos_after_FA","iscr_gng_resp_sqrt_z_score.neg_after_FH","iscr_gng_resp_sqrt_z_score.pos_after_FH")], function(x) shapiro.test(x)["p.value"]))


normality             <- unlist(normality[,1])
normality             <- data.frame(round(normality, digits = 2))
normality$condition   <- rownames(normality)
rownames(normality)   <- NULL
colnames(normality)   <- c("Shapiro_Wilk_p_value","Condition")
normality$Shapiro_Wilk_p_value[normality$Shapiro_Wilk_p_value < 0.001] <- "< 0.001"

kable(normality) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "left") 

ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x = iscr_gng_resp_sqrt_z_score)) + geom_density() + facet_wrap(response_type ~ valence) + ggtitle("Histogram SCR Square Root Trandformed in Conditions")
```

<br>

#### Normality of Single Trial SCR Data

For the single-trial data, Shapiro-Wilk is not suitable, as it always returns a significant result for such large samples (additionally, it can handle only samples up to 5000). Hence, we have to rely on visual inspection (see above) and values of skewness and kurtosis. Values for skewness and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010).

```{r test distribution SCR single}

normality <- round(data.frame(matrix(c(skewness(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_log_z_score),kurtosis(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_log_z_score),skewness(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_sqrt_z_score),kurtosis(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_sqrt_z_score)),nrow=2,ncol=2)),digits = 1)
rownames(normality) <- c("Skewness","Kurtosis")
colnames(normality) <- c("SCR Log Transformed Z-Standardized","SCR Square Root Transformed Z-Standardized")

kable(normality) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "left")
```

<br>


#### Summary

Normal distribution of sampling data can be assumed for square root transformed z-standardized SCR data. 

<br>

#### Definition of Non-Responders


Non-responder describes an individual who fails to show any or a certain (arbitrary) percentage of non-zero CRs (Lonsdorf et al., 2017). Here, participants lacking measurable SCR on >75% of all trials (or trials of one response condition) were classified as non-responders. All participants' SCRs exceeded this threshold so no participants were excluded. The proportion of zero responses was calculated based on all trials (also including CI and word outliers, as also recommended by JK).

```{r non-responders SCR}
df_aggregated_scr_response_type_incl_zero_resp <- ddply(data4mixedmodels[!is.na(data4mixedmodels$iscr_gng_resp),], 
                                                        .(subjectID, response_type), 
                                                        summarise,
                                                        event_counts_incl_zero_scr = length(subjectID))

df_aggregated_scr_response_type_excl_zero_resp <- ddply(data4mixedmodels[!is.na(data4mixedmodels$iscr_gng_resp) & data4mixedmodels$iscr_gng_resp > 0,], 
                                                        .(subjectID, response_type), 
                                                        summarise,
                                                        event_counts_excl_zero_scr = length(subjectID))

df_non_responder_per_response_type <- left_join(df_aggregated_scr_response_type_incl_zero_resp,df_aggregated_scr_response_type_excl_zero_resp, by = c("subjectID","response_type"))

df_non_responder_per_response_type$proportion_zero_responses <- round(100 - (df_non_responder_per_response_type$event_counts_excl_zero_scr / df_non_responder_per_response_type$event_counts_incl_zero_scr * 100),digits = 2)

df_non_responder_overall <-  ddply(df_non_responder_per_response_type,.(subjectID), summarise,
                                   number_all_events_incl_zero_scr = sum(event_counts_incl_zero_scr),
                                   number_all_events_excl_zero_scr = sum(event_counts_excl_zero_scr))    

df_non_responder_overall$proportion_zero_responses_overall    <- round(100 - (df_non_responder_overall$number_all_events_excl_zero_scr / df_non_responder_overall$number_all_events_incl_zero_scr * 100),digits = 2)

df_non_responder_per_response_type <- df_non_responder_per_response_type[order(df_non_responder_per_response_type$response_type),]
df_non_responder_per_response_type <- cbind(
df_non_responder_per_response_type[df_non_responder_per_response_type$response_type == "FA","proportion_zero_responses"], df_non_responder_per_response_type[df_non_responder_per_response_type$response_type == "FH","proportion_zero_responses"], df_non_responder_per_response_type[df_non_responder_per_response_type$response_type == "SH","proportion_zero_responses"], df_non_responder_per_response_type[df_non_responder_per_response_type$response_type == "CI","proportion_zero_responses"])

colnames(df_non_responder_per_response_type) <- c("proportion_zero_responses_SH", "proportion_zero_responses_FH", "proportion_zero_responses_FA","proportion_zero_responses_CI")

df_non_responder <- cbind(df_non_responder_overall[,c("subjectID","proportion_zero_responses_overall")],df_non_responder_per_response_type)
rownames(df_non_responder) <- NULL

df_non_responder_summary <- stat.desc(df_non_responder[,c(2:6)])[c(4,5,9,13),]

kable(df_non_responder, caption = "Zero Responses per Subject") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "left")

kable(round(df_non_responder_summary, digits = 2), caption = "Summary Zero Responses") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "left")
```

<br>
<br>
<br>

# LMM SCR 

I used a linear mixed model to evaluate whether the SCR differs depending on type of the response to the GNG stimulus. I prefer to analyze SCR by means of a **LMM on the square root transformed SCR data** instead of a GLMM on the raw SCR data. 

Most studies I found that used mixed models to analyze SCR data also used an LMMs (Yap et al., 2017; Harnett et al., 2015; Neiss et al., 2009), only one used a GLMM (Prechtl, "Adaptive Music Generation for Computer Games", 2016). The studies using the LMM used the **transformed but not z-standardized SCR as dependent variable**. This makes sense, as a random intercept for subject is included in the model, so z-standardization is not necessary anymore (using random intercept AND standardized data may lead to unterestimation of variance of random intercept). I also calculated a LMM with the z-standardized values. For this model, the LMM assumptions were met. As the results remained the same, I can also use the unstandardized values.

The square root transformed SCR values are not normally distributed. The distribution is a bit awkward, as it is cut at zero and contains many values of zero. We did not want to exclude the zeros, because they actually represent meaningful data. The non-normality of the data is not problematic, because for the LMM only the residuals have to be normally distributed, and this is approximately the case (kurtosis is 1.9, skewness is 0.96). However, the assumption of linearity / homogeneity is not met (due to the fact that the distribution is cut at 0). This seems indeed a bit better when using the square root transformed z standardized values for the LMM, but I still do not want to do that. After all, the results are the same and also the same as in the ANOVA.

The reasons why I do not want to calculate a GLMM for the SCR is that I would need to state the distribution my SCR data follow and spacify a linking function. I am not sure which distribution and linking function this is (as mentioned before, the distribution is strange). Gamma and inverse gaussian are some of the few options that I can use for GLMMs. They may also make sense (inverse Gaussian distribution is appropriate when the variance increases rapidly with the predicted values; gamma distribution is appropriate when the variance increases linearly with the predicted values (Fox, 2008)). Using the gamma distribution with glmer did not work for my data, because one of my estimated coefficients could be negative (see here: https://stats.stackexchange.com/questions/356053/the-identity-link-function-does-not-respect-the-domain-of-the-gamma-family). Using the inverse gaussian distribution with identity link worked (but I still have to add 1 to the SCR data to avoid values of zero). Still, I do not know whether this choice makes sense. I also tested inverse gaussian distribution with inverse square link function (1/mu^2) as done by Prechtl (2016). It has a lower AIC and gives the same result as the identity link, but I am not sure concerning the interpretation. Another reason why I do not prefer a GLMM here is that the assumption of normality of random effects for the GLMM is not met, as are the LMM assumptions. Model comparison with the *anova* function showed no significant difference between the GLMM and the LMM, but a much lower AIC/BIC for the GLMM. 

**Overview LMM:**

* Response variable: SCR (square root transformed but not z-standardized)

* Fixed effects: Response type (FA, FA, SH) 
    
* Random effects: Subjects (and Words)

<br>

#### Check Distribution 

Raw SCR data and the square root transformed SCR values are not normally distributed. The residuals of the LMM with the square root transformed SCR values are approximately normally distributed, allowing data analysis by means of an LMM (see below). 

```{r distribution LMM SCR, fig.width=10, fig.height=4, message=FALSE, results=FALSE}
par(mfrow = c(1,4))  
# raw scr -> not normally distributed
plot(density(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp), main = "Histogram SCR Raw")
qqp(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp, "norm", main = "Q-Q Plot SCR Raw", ylab = "sample quantiles", id = FALSE)

# square root transformed scr -> not normally distributed (distribution is cut at 0)
plot(density(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_sqrt), main = "Histogram Square Root Transformed SCR")
qqp(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$iscr_gng_resp_sqrt, "norm", main = "Q-Q Plot Square Root Transformed SCR", ylab = "sample quantiles", id = FALSE)
par(mfrow = c(1,1)) 

data4mixedmodels_words_scr$response_type <- factor(data4mixedmodels_words_scr$response_type, levels=c("SH","FH","FA"))

scr_raw_cond <- ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x = iscr_gng_resp)) + geom_density() + facet_wrap(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$response_type) + ggtitle("Histogram SCR Raw in Conditions")
scr_sqrt_cond <- ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x = iscr_gng_resp_sqrt)) + geom_density() + facet_wrap(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$response_type) + ggtitle("Histogram Square Root Transformed SCR in Conditions")

grid.arrange(scr_raw_cond,scr_sqrt_cond,nrow=1)
```


<br>

#### Define Contrasts

I chose sliding difference (aka "repeated") contrasts, which successively test neighboring factor levels against each other. See more information on that topic in the section "Word Categorization Data".

```{r contrasts LMM SCR}

# make categorical variables factors
data4mixedmodels_words_scr$response_type <- factor(data4mixedmodels_words_scr$response_type, levels=c("SH","FH","FA"))
data4mixedmodels_words_scr$valence       <- factor(data4mixedmodels_words_scr$valence)
data4mixedmodels_words_scr$condition     <- factor(data4mixedmodels_words_scr$condition, levels=c("neg_after_SH","pos_after_SH","neg_after_FH", "pos_after_FH", "neg_after_FA","pos_after_FA"))
data4mixedmodels_words_scr$subjectID     <- factor(data4mixedmodels_words_scr$subjectID)
data4mixedmodels_words_scr$word          <- factor(data4mixedmodels_words_scr$word)

# I use sliding contrast (reasons for decision see Evernote entry 2019_08_06)
contrasts(data4mixedmodels_words_scr$response_type) <- contr.sdif(3)
contrasts(data4mixedmodels_words_scr$valence)       <- contr.sdif(2)

# add contrast as numerical covariate via model matrix (in order to enter contrasts individually in model and hence to use ||)
mm_c    <- model.matrix( ~ response_type*valence, data4mixedmodels_words_scr) # stick in model matrix 6 columns

# attach to dataframe 
data4mixedmodels_words_scr[,(ncol(data4mixedmodels_words_scr)+1):(ncol(data4mixedmodels_words_scr)+6)] <- mm_c
names(data4mixedmodels_words_scr)[(ncol(data4mixedmodels_words_scr)-5):ncol(data4mixedmodels_words_scr)] <- c("Grand Mean", "FH_minus_SH","FA_minus_FH","pos_minus_neg", "FH_minus_SH:pos_minus_neg", "FA_minus_FH:pos_minus_neg")

# prepare labels for tables
pl <- c(
  "(Intercept)" = "Intercept",
  "valence2-1" = "Positive - Negative",
  "response_type2-1" = "FH - SH", 
  "response_type3-2" = "FA - FH",
  "iscr_gng_resp_sqrt_z_score" = "SCR",
  "response_type2-1:valence2-1" = "FH - SH x Positive - Negative", 
  "response_type3-2:valence2-1" = "FA - FH x Positive - Negative",
  "valence2-1:iscr_gng_resp_sqrt_z_score" = "Positive - Negative x SCR",
  "response_type2-1:iscr_gng_resp_sqrt_z_score" = "FH - SH x SCR", 
  "response_type3-2:iscr_gng_resp_sqrt_z_score" = "FA - FH x SCR",
  "response_type2-1:valence2-1:iscr_gng_resp_sqrt_z_score" = "FH - SH x Positive - Negative x SCR", 
  "response_type3-2:valence2-1:iscr_gng_resp_sqrt_z_score" = "FA - FH x Positive - Negative x SCR",
  "response_typeSH:valence2-1" = "SH: Positive - Negative",
  "response_typeFH:valence2-1" = "FH: Positive - Negative",
  "response_typeFA:valence2-1" = "FA: Positive - Negative",
  "response_typeSH:iscr_gng_resp_sqrt_z_score" = "SH: SCR",
  "response_typeFH:iscr_gng_resp_sqrt_z_score" = "FH: SCR",
  "response_typeFA:iscr_gng_resp_sqrt_z_score" = "FA: SCR", 
  "response_typeSH:valence2-1:iscr_gng_resp_sqrt_z_score" = "SH: Positive - Negative x SCR",
  "response_typeFH:valence2-1:iscr_gng_resp_sqrt_z_score" = "FH: Positive - Negative x SCR",
  "response_typeFA:valence2-1:iscr_gng_resp_sqrt_z_score" = "FA: Positive - Negative x SCR",
  "pos_minus_neg" = "Positive - Negative",
  "FH_minus_SH" = "FH - SH", 
  "FA_minus_FH" = "FA - FH",
  "FH_minus_SH:pos_minus_neg" = "FH - SH x Positive - Negative", 
  "FA_minus_FH:pos_minus_neg" = "FA - FH x Positive - Negative",
  "pos_minus_neg:iscr_gng_resp_sqrt_z_score" = "Positive - Negative x SCR",
  "FH_minus_SH:iscr_gng_resp_sqrt_z_score" = "FH - SH x SCR", 
  "FA_minus_FH:iscr_gng_resp_sqrt_z_score" = "FA - FH x SCR",
  "FH_minus_SH:pos_minus_neg:iscr_gng_resp_sqrt_z_score" = "FH - SH x Positive - Negative x SCR", 
  "FA_minus_FH:pos_minus_neg:iscr_gng_resp_sqrt_z_score" = "FA - FH x Positive - Negative x SCR")
```

<br>

#### Model Building

See which approach I chose for building the model in the section "Word Categorization Data".

<br>

#### Run LMM

For the SCR, the LMM containing the maximal random effects structure and correlation parameters was used. This is also the final model I get when I initially include (1 + FH_minus_SH + FA_minus_FH  | word) as random effect in the maximal model: rePCA shows that for word, all random effects including intercept explain zero variance. 


```{r build LMM SCR, echo = 2}

LMM_scr_no_val_final <- lmer(sqrt(iscr_gng_resp) ~ FH_minus_SH + FA_minus_FH + (1 + FH_minus_SH + FA_minus_FH  | subjectID), data=data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

tab_model(LMM_scr_no_val_final,dv.labels = "SCR [square root transformed (µS)]", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)
```
<br>
<br>

<br>

To test all three levels of	the factor response type against each other, we ran models with two different factor-level orders (i.e., FH, FA, SH and FA, FH, SH). 

```{r LMM SCR 2, echo = 13, fig.width=10, fig.height=6}

# make categorical variables factors
data4mixedmodels_words_scr$response_type <- factor(data4mixedmodels_words_scr$response_type, levels=c("SH","FA","FH")) 

# define contrasts
contrasts(data4mixedmodels_words_scr$response_type) <- contr.sdif(3)

# add contrast as numerical covariate via model matrix (in order to enter contrasts individually in model and hence to use ||)
mm_c    <- model.matrix( ~ response_type, data4mixedmodels_words_scr) # stick in model matrix 3 columns
data4mixedmodels_words_scr[,((ncol(data4mixedmodels_words_scr)+1):(ncol(data4mixedmodels_words_scr)+2))] <- mm_c[,c(2,3)]
names(data4mixedmodels_words_scr)[(ncol(data4mixedmodels_words_scr)-1):ncol(data4mixedmodels_words_scr)] <- c("FA_minus_SH", "FH_minus_FA")

LMM_scr_no_val_final2 <- lmer(sqrt(iscr_gng_resp) ~ FA_minus_SH + FH_minus_FA + (1 + FA_minus_SH + FH_minus_FA  | subjectID), data=data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

tab_model(LMM_scr_no_val_final2,dv.labels = "SCR [square root transformed (µS)]", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)

# undo change in order of factor levels and delete columns in df
data4mixedmodels_words_scr$response_type <- factor(data4mixedmodels_words_scr$response_type, levels=c("SH", "FH", "FA")) 
contrasts(data4mixedmodels_words_scr$response_type) <- contr.sdif(3)
data4mixedmodels_words_scr <- data4mixedmodels_words_scr %>%
                                  dplyr::select(-FA_minus_SH,-FH_minus_FA)
```

<br>

#### Summary LMM

The SCR differs significantly between FAs and FHs/SHs but not between FHs and SHs (but there is a trend). These results are  consistent with the ANOVA post-hoc tests. Important note: the trend was not present (neither for LMM not for ANOVA) when I did not exclude the word outliers for these analyses. In the GLMM, which shows mostly consistent results, FH vs SH is not significant or a trend even after exclusion of trials with word outlier.

<br>

#### Check Assumptions LMM

Some assumptions (1 and 2) seem violated here. The plot of the residuals versus the predicted variable looks better when z-standardized and square root transformed SCR data are used in the LMM instead of only square root transformed. But I do not really want to use z-standardization here, as the random effects already account for variability between subjects. For both dependent variables, the results are the same and they are also quite consistent with the ANOVA. So it seems ok to use square root transformed SCR data, even though some assumptions might be violated.


**Assumption 1 - Linearity**: dependent variable linearly related to the fixed factors, random factors, and covariates (dv is result of a linear combination of the predictors)

* not so ok (see plot of the residuals versus the predicted variable) 

**Assumption 2 - Homogeneity of Variance**: residuals have constant variance (variability of data approximately equal across range of predicted values)

* not so ok? (see plot of the residuals versus the predicted variable) 

**Assumption 3 - Independence**: not necessary to check, because there is not more than one predictor

**Assumption 4 - Normality of Residuals**: residuals are approximately normally distributed

* is ok (see Q-Q plot of residuals: a bit off at the extremes, but that's often the case; doesn't look too bad; skewness and kurtosis is < 2, which is ok)

```{r LMM SCR Assumptions}
par(mfrow = c(1,2))
# LINEARITY & HOMOGENEITY OF VARIANCE
plot(fitted(LMM_scr_no_val_final), residuals(LMM_scr_no_val_final), main = "Residuals vs Predicted Values", xlab = "Fitted Values", ylab = "Residuals")  # plot residuals against the fitted values
abline(h = 0, lty = 2)
lines(smooth.spline(fitted(LMM_scr_no_val_final), residuals(LMM_scr_no_val_final)))
# scatter looks not like a random pattern (looks better when using the z score, but I actually do not want to do that)
# homogeneity of variance seems to be given but not linearity
# solid line covers the dashed line -> best-fit line fits well


# NORMALITY OF RESIDUALS
qqnorm(resid(LMM_scr_no_val_final))
qqline(resid(LMM_scr_no_val_final)) # a bit off at the extremes, but that's often the case; again doesn't look too bad
par(mfrow = c(1,1))

skewness <- round(skewness(resid(LMM_scr_no_val_final)),digits = 2) # 0.96 -> is ok
kurtosis <- round(kurtosis(resid(LMM_scr_no_val_final)),digits = 2) # 1.85 -> is ok

print(paste("Skewness Residuals Distribution:", skewness))
print(paste("Kurtosis Residuals Distribution:", kurtosis))

```

<br>
<br>
<br>

# Comparison with ANOVA

I calculated an univariate two-way repeated measures ANOVA, containing the within-subjects response type (levels FA, FH, SH). There is a main effect of Response Type. 

```{r SCR ANOVA}

options(contrasts=c("contr.sum","contr.poly")) # to adhere to the sum-to-zero convention for effect weights, always do this before running ANOVAs in R. This matters sometimes (not always). If I do not do it, the sum of squares calculations may not match what I get e.g. in SPSS

 df_aggregated_scr_response_type$response_type <- factor(df_aggregated_scr_response_type$response_type)                                    
 df_aggregated_scr_response_type$subjectID     <- factor(df_aggregated_scr_response_type$subjectID)
 anova_SCR <- aov_ez(id = "subjectID", dv = "iscr_gng_resp_sqrt_z_score", data = df_aggregated_scr_response_type, within = "response_type")
# summary(anova_SCR) to see results of Mauchly's Sphericity Test -> sign. -> sphericity not given


# plot ANOVA
# afex_plot(anova_SCR, x = "response_type", trace = "word_valence", error = "within", mapping = c("color", "fill")) + theme_light()

 # display output
apa_anova_SCR <- apa_print(anova_SCR)
kable(apa_anova_SCR$table, caption = "ANOVA SCR")  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")
```

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

#### Post Hoc Tests

For within-subject repeated measures ANOVAs, pairwise t tests with an adjustment for multiple comparisons (e.g. Bonferroni or Holm) are recommended (see Evernote, 2019_07_26). The violation of sphericity in within subjects designs makes the error term incorrect and the Tukey approach problematic. Pairwise t tests with Holm adjustments are presented here.


**Main effect of Response Type on SCR**
According to pairwise comparison with t test, FA > FH and FA > SH and trend that FH > SH. This is exactly in line with the LMM results. (Using the Tukey post hoc adjustment FH = SH). The results of the LMM and the ANOVA including response type are consistent.


```{r SCR ANOVA post hoc tests}

# using pairwise comparison with t tests (error term is by default not pooled, which is the right choice for within Ss ANOVAs)
anova_SCR_posthoc_response_type <- pairwise.t.test(df_aggregated_scr_response_type$iscr_gng_resp_sqrt_z_score,df_aggregated_scr_response_type$response_type, p.adjust.method="holm",paired=TRUE)

kable(anova_SCR_posthoc_response_type$p.value, caption = "ANOVA SCR: Post Hoc Test Response Type")  %>%
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "left")  %>%
 footnote(general = "P value adjusted with Holm method.")
```

<br>

#### Testing Normality of Residuals

Residuals of the square root transformed SCR are normally distributed in each condition, except for FA (pos). But even in this condition, the deviation is not severe.

```{r test normality residuals, fig.width=10, fig.height=8}

residuals_scr                         <- data.frame(residuals(anova_SCR$lm)) 
residuals_scr_shapiro                 <- do.call(rbind, lapply(residuals_scr[,], function(x) shapiro.test(x)["p.value"]))
residuals_scr_shapiro                 <- unlist(residuals_scr_shapiro[,1])
residuals_scr_shapiro                 <- data.frame(round(residuals_scr_shapiro, digits = 2))
residuals_scr_shapiro$condition       <- rownames(residuals_scr_shapiro)
residuals_scr_shapiro                 <- data.frame(cbind(residuals_scr_shapiro$condition, round(residuals_scr_shapiro[,1], digits = 3)), stringsAsFactors = FALSE)
colnames(residuals_scr_shapiro)       <- c("Condition","Shapiro_Wilk_p_value")
residuals_scr_shapiro$Shapiro_Wilk_p_value[residuals_scr_shapiro$Shapiro_Wilk_p_value < 0.001] <- "< 0.001"

kable(residuals_scr_shapiro, caption = 'Normality Residuals of Transformed SCR') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "left") 


hist_residuals_scr <- ggplot(gather(residuals_scr, cols, value), aes(x = value)) + 
       geom_histogram(color="gray33", fill = "lightcoral", binwidth = 0.05) + facet_grid(.~cols) +
       labs (title = "Histogram Residuals SCR", x = "Residual SCR", y ="Count") + 
       theme(plot.title = element_text(hjust = 0.5)) 

qqplot_residuals_scr <- ggplot(gather(residuals_scr, cols, value), aes(sample = value)) + 
  stat_qq(color = "lightcoral") +
  facet_grid(.~cols) +
  labs (title = "Q-Q-Plot Residuals SCR", x = "Theoretical Quantiles", y ="Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(hist_residuals_scr,qqplot_residuals_scr,nrow = 2)
```


<br>

#### Evaluating Assumptions ANOVA

* **Assumption #1:** Dependent variable interval or ratio variable 

* **Assumption #2:** Balanced design (each subject has to have a value in each condition) 

* **Assumption #3:** No dependency in the scores between participants (dependency can exist only across scores for individuals) 

* **Assumption #4:** Distribution of residuals of the dependent variable in each level of the within-subjects factor is approximately normally distributed 

* **Assumption #5:** Sphericity  (variance of difference scores computed between any two levels of a within subject factor must be equal) (Mauchly's Sphericity Test and Greenhouse Geisser Correction applied to dfs)

* **Assumptions 1, 2, 3, 4 are fulfilled for SCR (assumption 4 is mostly fulfilled)**

* **Assumption 5 (sphericity) is violated but corrected for **



<br>
<br>
<br>

# Plots

```{r plot, fig.width=10, fig.align='center'}

plot_scr <- ggplot(df_aggregated_over_subjects_resptype,aes (x = response_type,y = iscr_gng_resp_sqrt_z_score)) +
  geom_bar(stat="identity", position=position_dodge(), fill = "steelblue4") +                                                       
  geom_errorbar(aes(ymax = iscr_gng_resp_sqrt_z_score + ci, ymin = iscr_gng_resp_sqrt_z_score - ci), position = position_dodge(width=0.95), width=0.1) +    
  ggtitle("Skin Conductance Response") +                                                                            
  xlab("Response Type") + ylab("SCR [square root and z transformed (µS)]") +                                                                  
  guides(fill=guide_legend(title="Word Valence")) +                                                                         
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.title = element_text(size = 18, hjust = 0.5)) +                                                
  theme(axis.text=element_text(size=15)) +
  theme(legend.title=element_text(size=15)) +
  theme(legend.text=element_text(size=15)) +
  theme(plot.title = element_text(size = 20, hjust = 0.5)) +
  theme(legend.position="bottom") +
  coord_cartesian(ylim = c(-0.15,0.6)) +
  geom_signif(y_position=c(0.55,0.55), xmin=c(1.02, 2.02), xmax=c(1.98, 2.98),annotation=c("NS","***"), tip_length=0.02)
 # ggsave("scr.png", plot_scr, width = 18, height = 16, units = "cm")


plot_scr_val <- ggplot(df_aggregated_over_subjects,aes (x = response_type,y = iscr_gng_resp_sqrt_z_score, fill = word_valence)) +
  geom_bar(stat="identity", position=position_dodge()) +                                                       
  geom_errorbar(aes(ymax = iscr_gng_resp_sqrt_z_score + ci, ymin = iscr_gng_resp_sqrt_z_score - ci), position = position_dodge(width=0.95), width=0.1) +    
  ggtitle("Skin Conductance Response") +                                                                            
  xlab("Response Type") + ylab("SCR [square root and z transformed (µS)]") +                                                                  
  guides(fill=guide_legend(title="Word Valence")) +                                                                         
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.title = element_text(size = 18, hjust = 0.5)) +                                                
  theme(axis.text=element_text(size=15)) +
  theme(legend.title=element_text(size=15)) +
  theme(legend.text=element_text(size=15)) +
  theme(plot.title = element_text(size = 20, hjust = 0.5)) +
  theme(legend.position="bottom") +
  scale_fill_manual(values=c("steelblue4", "slategray1")) +
  coord_cartesian(ylim = c(-0.15,0.6)) +
  geom_signif(y_position=c(0.55,0.55), xmin=c(1.02, 2.02), xmax=c(1.98, 2.98),annotation=c("NS","***"), tip_length=0.02)

grid.arrange(plot_scr,plot_scr_val,nrow=1)
```
<br>
Error bars depict 95% CI. Values are adjusted for within-participant designs following Morey (2008). Calculation of error bars was based on the non-aggregated data, as this better reflects the LMM approach. Computing error bars based on the aggregated data per subject yields different results, as in the mean of the group means it would not be considered that subjects have a different number of events per condition; this more resembles the ANOVA approach, because it assumes that each subject mean is equally reliable (relies on the same amount of information), which is not true.

<br>

#### Tables Mean and CI/SE

```{r descriptive}

# this shows the data calculated based on the non aggregated (single trial) data. This corresponds to the LMM approach and my plots.
table_scr <- df_aggregated_over_subjects_resptype[,c("response_type","N","iscr_gng_resp_sqrt_z_score","sd","se","ci")]
table_scr[,c("iscr_gng_resp_sqrt_z_score","sd","se","ci")] <- round(table_scr[,c("iscr_gng_resp_sqrt_z_score","sd","se","ci")], digits = 2)
table_scr <- t(table_scr)
rownames(table_scr) <- c("","N","M","SD","SE","95% CI")

kable(table_scr[,c(3,2,1)], caption = 'SCR [square root and z transformed (µS)]') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")

table_scr_val <- df_aggregated_over_subjects[,c("condition","N","iscr_gng_resp_sqrt_z_score","sd","se","ci")]
table_scr_val[,c("iscr_gng_resp_sqrt_z_score","sd","se","ci")] <- round(table_scr_val[,c("iscr_gng_resp_sqrt_z_score","sd","se","ci")], digits = 2)
table_scr_val <- t(table_scr_val)
rownames(table_scr_val) <- c("","N","M","SD","SE","95% CI")
table_scr_val <- table_scr_val[,c(3,6,2,5,1,4)]

kable(table_scr_val, caption = 'SCR [square root and z transformed (µS)]') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")
```

Note: The values here are based on the non-aggregated single trial data. This corresponds to the LMM approach and my plots.

<br>

#### Estimated Marginal Means Plot/Tab

Error bars depict 95% CI.

```{r estimated mean plots, fig.width=5, fig.height=4, fig.align='center'}

emm_options(pbkrtest.limit = 7139)
# recalculate model, as needed for emmeans
LMM_scr_no_val_final <- lmer(sqrt(iscr_gng_resp) ~ response_type + (1 + FH_minus_SH + FA_minus_FH  | subjectID), data=data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

Estimated_Means_SCR = as.data.frame(emmeans(LMM_scr_no_val_final, ~ response_type, type = "response"))
Estimated_Means_SCR$response_type <- factor(Estimated_Means_SCR$response_type, levels = c("SH","FH","FA"))

plot_est_means_scr <- ggplot(Estimated_Means_SCR,aes (x = response_type,y = response)) +
  geom_bar(stat="identity", position=position_dodge()) +       
  geom_errorbar(aes(ymax = upper.CL, ymin =lower.CL), position = position_dodge(width=0.9), width=0.2,size=0.3) +    
  theme_apa(base_size = 14) + # from papaja package 
  scale_y_continuous(expand = c(0, 0)) +        # neat axes ends
  labs(x="Response Type", y="Predicted SCR (µS)") +
  coord_cartesian(ylim = c(0,0.4)) +
  scale_fill_grey(start = 0.40, end = 0.6) +
  geom_signif(y_position=c(0.38,0.35), xmin=c(1,2), xmax=c(3,3),
              annotation=c("***","**"), tip_length=0.01) 
                                            
plot_est_means_scr
ggsave("figure_scr.jpg",width = 13, height = 9, units = "cm", dpi=500)


kable(cbind(Estimated_Means_SCR[,1],round(Estimated_Means_SCR[,c(2:6)], digits = 2)), col.names = c("response type", "Mean", "SE", "df", "asymp.LCL", "asymp.UCL"), caption = 'Estimated Marginal Means SCR') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")
```


<br>
<br>
<br>

# Relation Priming - SCR

I used a linear mixed model to evaluate whether there is a relation between word categorization RT (= priming) and SCR.

In order to do that, my first approach was to calculate a LMM including SCR as dependent variable and RT facilitation as predictor. Using this model, I found an interaction between SCR and priming effect (RT facilitation) and word valence (more SCR is related to less RT facilitation for a neg word after a false alarm). But this LMM analysis was very problematic. Defining RT facilitation / priming effect on single trial level is not possible. I used mean RT in that condition minus RT in that trial, but of course this difference is more influenced by word length, frequency, etc. than by priming. 

Hence, I decided to include SCR as a predictor in the LMM predicting word categorization RT. This may be a bit unusal, as the predictor event precedes the criterion, but it solves the problem described. This analysis allowed investigating the relation between SCR and priming while controlling for the variance caused by different words on categorization time.  

The model was constructed like the word categorization LMM but included SCR as continuous predictor in all fixed and random effects. I decided to report two separate models for word rt and word rt with SCR. For the first model, I included CI to show that there is no priming effect after CI. For the second model, I excluded CI, because there is no valid SCR in these trials (this is the only condition not confounded by a motor response) and I want to exclude CI from all SCR analyses. A relation between priming and SCR would be  would be evident if there was a 3 way interaction between response condition, valence, and SCR. 

I calculated a LMM instead of a GLMM, because I also did so in the analysis of word categorization without SCR as predictor. In order to check whehther a GLMM model yielded the same outcomes, I also analyzed the relation between word categorization RT by means on a GLMM. As in the original word categorization mixed model, I chose inverse gaussian distribution and the identity link funktion (recommended by Lo & Andrews, 2015) for the GLMM. The results were quite the same (for more detail, see Summary LMM).


**Overview LMM:**

* Response variable: RT (inverse transformed)

* Fixed effects: Response type (SH, FH, FA) and Word Valence (pos, neg)
    
* Random effects: Subjects and Words

* Continuous predictor: Square root transformed and z-standardized SCR (response-locked to GNG-response) (it is recommended that continuous predictors shall be scaled)


As in all other analyses on SCR data, I only included SH, FH, FA trials that had no invalid gng rt or word rt (these trials were consistently excluded for all analyses), were followed by correct word classification and were not preceded or followed by an incorrect response or wrong key in gng or word classification.



<br>

#### Check Distribution 

Also in this subset of the data, the transfomed Word RT Inverse is normally distributed, allowing data analysis by means of an LMM (the residuals of this LMM are approximaltely normally distributed as well). 

```{r distribution LMM Priming - SCR, fig.width=10, fig.height=6}

# distribution transformed rt -> normally distributed, also in the different conditions
par(mfrow = c(1,2))  # use par function in base graphics approach to arrange plots side by side here; grid.arrange only works for ggplot grahpics
plot(density(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$word_rt_inverse), main = "Histogram Word RT Inverse")
qqp(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,]$word_rt_inverse, "norm", main = "Q-Q Plot Word RT Inverse", ylab = "sample quantiles", id = FALSE)
par(mfrow = c(1, 1)) # reset "par" parameter
ggplot(data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], aes(x = word_rt_inverse)) + geom_density() + facet_wrap(response_type ~ valence) + ggtitle("Histogram Word RT Inverse in Conditions")
```
<br>

#### Define Contrasts

I chose sliding difference (aka "repeated") contrasts, which successively test neighboring factor levels against each other. See more information on that topic in the section "Word Categorization Data".

```{r contrasts LMM Priming - SCR}
# run constrast code from SCR LMM
```

<br>

#### Model Building

See which approach I chose for building the model in the section "Word Categorization Data".

<br>

#### Run LMM

For the Priming - SCR LMM, the  maximal random effects were reduced of the random effect explaining zero variance: all interactions response_type x SCR for word and subject and interactions response_type x valence x SCR and valence x SCR for subject; with correlation parameters set to zero. Note that the model using the maximal random structure (including correlations) also converged and yielded the same estimates (for the fuller model, only the trend of main effect SCR was less pronounced). 
It would be *not* ok to not include SCR in random structure, because this predictor is of major interest and we want to make valid inferences from the  corresponding fixed effect. It is not a covariate for which we do not need to generalize the effect and for which we only want to account for the variance it explains.

```{r build LMM Priming SCR, echo = 2}

LMM_rt_scr_final <- lmer(word_rt_inverse ~ FH_minus_SH + FA_minus_FH + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + iscr_gng_resp_sqrt_z_score + FH_minus_SH:iscr_gng_resp_sqrt_z_score + FA_minus_FH:iscr_gng_resp_sqrt_z_score + pos_minus_neg:iscr_gng_resp_sqrt_z_score + FH_minus_SH:pos_minus_neg:iscr_gng_resp_sqrt_z_score + FA_minus_FH:pos_minus_neg:iscr_gng_resp_sqrt_z_score + (1 + FH_minus_SH + FA_minus_FH + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + iscr_gng_resp_sqrt_z_score || subjectID) + (1 + FH_minus_SH + FA_minus_FH + iscr_gng_resp_sqrt_z_score || word), data=data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

tab_model(LMM_rt_scr_final, order.terms = c(1,2,3,4,6,7,5,10,8,9,11,12), dv.labels = "RT [-1000/RT(ms)]] Word Categorization", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)
```

<br>

Akin to the ANOVA omnibus test, it can be tested whether the interaction response type x valence x SCR improves the model fit significantly. It does not. Thus, there is no omnibus interaction effect. The SCR does not modulate the priming effect and there is no relation between priming and SCR. This is also evident when the interaction is resolved in the model nested within each response condition (see below).

<br>

#### Break Down the Interaction

I tested the effect of valence, of scr, and of valence*scr within each response condition by calculating a LMM that is nested in the final model above. 

```{r nested LMM Priming SCR, echo = 2}
# valence, scr, valence*scr within response condition 
LMM_rt_scr_nested <- lmer(word_rt_inverse ~ response_type/(valence*iscr_gng_resp_sqrt_z_score) + (1 + FH_minus_SH + FA_minus_FH + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + iscr_gng_resp_sqrt_z_score  || subjectID) + (1 + FH_minus_SH + FA_minus_FH + iscr_gng_resp_sqrt_z_score  || word), data=data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

tab_model(LMM_rt_scr_nested, order.terms = c(1,2,3,4,5,6,7,8,9,10,11,12), dv.labels = "RT [-1000/RT(ms)]] Word Categorization", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)
```

<br>

#### Summary LMM

There is no relation between priming effect and the SCR (no significant interaction after any response type). Interestingly, there is a significant interaction between FAs and SCR, suggesting that for False Alarms, slower responses to the words are related to a higher skin conductance response. This would mean that more PES is related to more SCR. This was none of my hypotheses but is quite interesting and may be reported as post hoc finding in the paper. 
The results of the GLMM were quite the same. The only difference is that in the GLMM there is also a significant difference between response types FH_vs_SH and a significant effect of SCR (in the LMM the p value is at .063) in the full model. The nested GLMM does not converge anymore. In the past, it did converge. Recently, I decided to exclude word outlier trials also from the z standardization of the SCR. I rechecked, the convergence problems are not due to this change. I have no idea, what changed this. At least the results of no the non converging nested GLMM are consistent with the nested LMM (only one difference: in the nested GLMM there is now a trend for FH x SCR x valence).  


<br>

#### Check Assumptions LMM

**Assumption 1 - Linearity**: dependent variable linearly related to the fixed factors, random factors, and covariates (dv is result of a linear combination of the predictors)

* is ok (see plot of the residuals versus the predicted variable) 

**Assumption 2 - Homogeneity of Variance**: residuals have constant variance (variability of data approximately equal across range of predicted values)

* is ok (see plot of the residuals versus the predicted variable) 

**Assumption 3 - Independence**: not necessary to check, because there is not more than one predictor

**Assumption 4 - Normality of Residuals**: residuals are approximately normally distributed

* is ok (see Q-Q plot of residuals: a bit off at the extremes, but that's often the case; doesn't look too bad; skewness and kurtosis is approx. < 2, which is ok)

```{r test LMM assumptions, fig.width=10, fig.height=4, fig.align='center'} 
par(mfrow = c(1,2))  # use par function in base graphics approach to arrange plots side by side here; grid.arrange only works for ggplot grahpics

plot(fitted(LMM_rt_scr_final), residuals(LMM_rt_scr_final), xlab = "Fitted Values", ylab = "Residuals", main = "Residuals versus Predicted Variable")  # plot residuals against the fitted values
abline(h = 0, lty = 2)
lines(smooth.spline(fitted(LMM_rt_scr_final), residuals(LMM_rt_scr_final)))

qqnorm(resid(LMM_rt_scr_final))
qqline(resid(LMM_rt_scr_final)) 

par(mfrow = c(1, 1)) # reset "par" parameter

skewness <- round(skewness(resid(LMM_rt_scr_final)),digits = 2) 
kurtosis <- round(kurtosis(resid(LMM_rt_scr_final)),digits = 2) 

print(paste("Skewness Residuals Distribution:", skewness))
print(paste("Kurtosis Residuals Distribution:", kurtosis))
```




<br>

#### Correlation SCR - Priming after FA

By calculating the correlation [see below], I wanted to check whether the unexpected relation found in the original/problematic LMM (more SCR is related to less RT facilitation for a neg word after a false alarm) can be seen on the level of aggregated data as well. 
The significance of this correlation depends on which trials of a subject are used for the z standardization. Previously, I did the scaling based on all trials. Then, I have an almost sign. correlation between SCR after FA and rt_priming_after_FA (p = 0.073). If the scaling is based only on trials that are used for SCR analysis later, the correlation is not almost significant anymore (p = 0.2). As bivariate normality is almost given (see last line), a Pearson correlation is calculated. As can be seen in the scatterplot, the correlation is not driven by a specific outlier.

```{r corr SCR priming, fig.width = 6, fig.height = 4}

df_wide_scr$subjectID     <- factor(df_wide_scr$subjectID)  # need to have same type for merging

df_wide_scr <- left_join(df_wide_scr,df_wide, by = "subjectID")
ggplot(df_wide_scr, aes(x = rt_priming_after_FA, y = iscr_gng_resp_sqrt_z_score.FA)) +
    geom_point() +
    geom_smooth(method='lm') +
    ggtitle("Scatterplot SCR after FA and Priming after FA") +
    theme(plot.title = element_text(hjust = 0.5))


cor.test(df_wide_scr$iscr_gng_resp_sqrt_z_score.FA,df_wide_scr$rt_priming_after_FA, paired = TRUE)

shapiro_corr <- unlist(mshapiro.test(t(df_wide_scr[,c("iscr_gng_resp_sqrt_z_score.FA","rt_priming_after_FA")]))["p.value"])
print(paste("p value of Shapiro Wilk Test for Bivariate Normality:", round(shapiro_corr, digits = 3)))

```

<br>
<br>
<br>

# Control Analyses

My task design is not optimal for SCR analyses. The short ISIs are problematic for analyzing the SCR. Many events (word presentation, word categorization, GNG presentation, GNG reaction) overlap with the window of the exported mean SCR amplitude (1-4 sec after the GNG response). SCR studies often use relatively long interstimulus intervals (ISI) of 6 to 12 seconds or more between trials to make sure skin conductance has returned to baseline before a new trial starts. A fast sequence of stimuli inevitably leads to superimposed SCRs which suffer from distorted amplitudes and temporal characteristics (Boucsein et al., 2012, Dawson et al., 2007). Attempts to correct for this and apply SCR measurement to much shorter ISIs (i.e., 2-3s) have relied on mathematical deconvolution models (Alexander et al., 2005; Benedek & Kaernbach, 2010). The CDA approach claims to be able to deal with ITI/ISIs as short as 2 s (Benedek and Kaernbach, 2010). My overlap might be too severe, as the time lag is too short. Furthermore, the intervals between events should be jittered. This is not true for my interval between gng response and word presentation (is fixed at 300 ms). 

Benedek & Kaernbach (2010) wrote: *If the biasing SCR preceded or followed the elicited SCR by more than about 3 s, the impulses of the phasic driver were fully disjunct and the impulse elicited by the biasing SCR resided outside the response window. Responses to stimuli at an ISI of two seconds can be distinguished on the basis of two distinct peaks; however, the phasic driver responses are not fully distinct. Simulation data demonstrated that the amplitude of an elicited SCR will be affected by a biasing SCR with inter-SCR latencies of less than 3 s due to the overlap of driver impulses. This influence increases gradually with the increasing proximity of the second SCR. For ISIs as low as two seconds, not only do SCRs overlap but also the ranges of expected onset latencies (according to most definitions of response windows). An unambiguous classification of SCRs to either stimulus thus appears impossible due to the variability of the electrodermal response.*

As in our design the ISI duration was short, we tested whether there are any differences in SCR responding for trials in which the gng response was followed by a negative vs. a positive word. As we do not observe differences between both valence conditions (see control analyses below), we conclude that SCR responses to the word presentation are not confounding our SCR related to the gng responses. 
Furthermore, the additional responses that may elicit confounding SCRs within the time window of interest (1-4 sec after gng response) were kept constant. Only trials that contained a correct word response and were followed and preceded by a correct gng an word response were included in the analysis. According to Benedek & Kaernbach (2010), keeping events constant 3 seconds before / after event of interest is sufficient. So it should suffice to keep the gng and word responses in the previous and preceding trial constant. 

<br>

#### Influence of Word Valence on SCR

To check whether the SCR is affected by the word following the GNG response, I also conducted a LMM and an ANOVA including the factors response type and valence of the following word (levels neg, pos). 
For the LMM, the  maximal random effects were reduced of the random effect explaining zero variance “FA - FH” and "Intercept" for word and "FH - SH x pos - neg" for subject; with correlation parameters set to zero. Note that the model using the maximal random structure (i.e., including “FA - FH” and "Intercept" for word and "FH - SH x pos - neg" for subject) also converged and yielded the same estimates. As long as the maximal model were to yield the same estimates as a justifiably reduced parsimonious model, there actually would be no need for model reduction (Matuschek 2017). For sake of simple explainability of the model selection, I decided to maintain the procedure to remove zero variance components. 
The results can be reported as a footnote: There is still a main effect of Response Type. There is no main effect of valence or an interaction of valence and response type. Hence, the SCR is not influenced by valence of following word. The results of the LMM and the ANOVA including response type and valence are mostly consistent (as before, FH vs. SH is a trend in the LMM, but in the ANOVA it gets significant here).

<br>
```{r test effect of valence on SCR}

# LMM
LMM_scr_final <- lmer(iscr_gng_resp_sqrt ~ FH_minus_SH + FA_minus_FH + pos_minus_neg + FH_minus_SH:pos_minus_neg + FA_minus_FH:pos_minus_neg + (1 + FH_minus_SH + FA_minus_FH + pos_minus_neg + FH_minus_SH:pos_minus_neg || subjectID) + (0 + FH_minus_SH  || word), data=data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

tab_model(LMM_scr_final,order.terms = c(1,2,6,5,4,3), dv.labels = "SCR [square root and z transformed (µS)]", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)


# ANOVA
 df_aggregated_scr_condition$response_type <- factor(substr(df_aggregated_scr_condition$condition, 11, 12))                                     
 df_aggregated_scr_condition$word_valence  <- factor(substr(df_aggregated_scr_condition$condition, 1, 3))
 df_aggregated_scr_condition$subjectID     <- factor(df_aggregated_scr_condition$subjectID)
 anova_SCR_val <- aov_ez(id = "subjectID", dv = "iscr_gng_resp_sqrt_z_score", data = df_aggregated_scr_condition, within = c("response_type","word_valence"))
# summary(anova_SCR_val) to see results of Mauchly's Sphericity Test -> sign. -> sphericity not given

apa_anova_SCR_val <- apa_print(anova_SCR_val)
kable(apa_anova_SCR_val$table, caption = "ANOVA SCR with Factors Response Type and Valence")  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_left")

anova_SCR_val_posthoc_response_type <- pairwise.t.test(df_aggregated_scr_condition$iscr_gng_resp_sqrt_z_score,df_aggregated_scr_condition$response_type, p.adjust.method="holm",paired=TRUE)

 kable(anova_SCR_val_posthoc_response_type$p.value, caption = "Post Hoc Test Response Type")  %>%
   kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = F, position = "float_right")  %>%
   footnote(general = "P value adjusted with Holm method.")
```
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

#### Word Valence after/bef. FA, FH, SH

Checking the frequency of the word valence in the same, the preceding, and the subsequent trial is acutally not necessary because the SCR is not influenced by valence of following word. This can be seen in the ANOVA / LMM including the factor valence and in the bar plot. 

Nevertheless, I checked whether in the trials entered in the analyses, the number of positive and negative words following the different GNG response types is the same. There is no significant difference in the number of occurence of pos / neg words following the false alarms, fast hits, or slow hits that are entered in the analysis. The reason why one might possibly expect a difference after false alarms is that I only use correctly classified words for these analyses (to avoid overlap with further error-related SCR increases) and subjects tend to have decreased accuracy for positive words after false alarms. 

```{r test word valence paralellization}

descriptive_statistics[,c("for_SCR_count.neg_after_FA","for_SCR_count.pos_after_FA","for_SCR_count.neg_after_FH","for_SCR_count.pos_after_FH","for_SCR_count.neg_after_SH","for_SCR_count.pos_after_SH")] <- round(descriptive_statistics[,c("for_SCR_count.neg_after_FA","for_SCR_count.pos_after_FA","for_SCR_count.neg_after_FH","for_SCR_count.pos_after_FH","for_SCR_count.neg_after_SH","for_SCR_count.pos_after_SH")], digits = 1)

kable(descriptive_statistics[(2:3),c("for_SCR_count.neg_after_FA","for_SCR_count.pos_after_FA","for_SCR_count.neg_after_FH","for_SCR_count.pos_after_FH","for_SCR_count.neg_after_SH","for_SCR_count.pos_after_SH")], caption = "Count") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")


t_test_valence_after_FA <- t.test(df_wide_scr$for_SCR_count.neg_after_FA, df_wide_scr$for_SCR_count.pos_after_FA,paired=TRUE)
t_test_valence_after_FH <- t.test(df_wide_scr$for_SCR_count.neg_after_FH, df_wide_scr$for_SCR_count.pos_after_FH,paired=TRUE)
t_test_valence_after_SH <- t.test(df_wide_scr$for_SCR_count.neg_after_SH, df_wide_scr$for_SCR_count.pos_after_SH,paired=TRUE)

t_tests  <- rbind(data.frame(cbind(round(t_test_valence_after_FA$statistic,digits = 3),round(t_test_valence_after_FA$p.value,digits = 3))),data.frame(cbind(round(t_test_valence_after_FH$statistic,digits = 3),round(t_test_valence_after_FH$p.value,digits = 3))),data.frame(cbind(round(t_test_valence_after_SH$statistic,digits = 3),round(t_test_valence_after_SH$p.value,digits = 3))))


colnames(t_tests) <- c("t(29)","p value")
rownames(t_tests) <- NULL
t_tests$condition <- c("after FA","after FH","after SH")

kable(t_tests[,c("condition","t(29)","p value")], caption = "t tests count pos neg words")  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width = T, position = "left")
```

<br>

#### Influence Fix Cross Color on SCR

I cannot evaluate the effect of fixation cross color in a convincing way. For doing this, I would need to include trials with white and red crosses that were only preceded and followed by white crosses. If I did this, only 1205 trials remain for the analysis. In such analysis, there indeed is a trend for the effect of color of fixation cross on SCR. But this analysis is based on a tiny subset of the original data and not representative.

Thus, I also checked whether the color of fixation cross affects the SCR in current trial (independent of color of fix cross in previous or following trial; note that the time window of SCR is much longer than current trial, leading to overlapping effects of subsequent fixation crosses). In this analysis, which allowed overlapping effects of fixation crosses of different color, the color of the fixation cross does not affect the SCR in the current trial.


```{r effect fixation cross}
data4mixedmodels_words_scr$color_cross <- NA   

# First trial of each block starts with  white fixcross
data4mixedmodels_words_scr[data4mixedmodels_words_scr$trial == 1 | data4mixedmodels_words_scr$trial == 29 | data4mixedmodels_words_scr$trial == 101 | data4mixedmodels_words_scr$trial == 173 | data4mixedmodels_words_scr$trial == 201 | data4mixedmodels_words_scr$trial == 273 | data4mixedmodels_words_scr$trial == 345 | data4mixedmodels_words_scr$trial == 373 | data4mixedmodels_words_scr$trial == 445,]$color_cross <- 0 

# Which rows satisfy the criteria? which rows are fast hits?
indices_FH    <- which(data4mixedmodels_words_scr$gng_resp == 41)  
indices_SH_FA <- which(data4mixedmodels_words_scr$gng_resp > 41)  # specify second condition 

# Assign 0 to each row following FH except very last row
data4mixedmodels_words_scr[1:(nrow(data4mixedmodels_words_scr)+1),]$color_cross[indices_FH+1] <- 0     # white cross in FH trials (CI not included here)
data4mixedmodels_words_scr[1:(nrow(data4mixedmodels_words_scr)+1),]$color_cross[indices_SH_FA+1] <- 1  # red cross in FA / SH trials

# Delete empty rows that were assigned to df now
data4mixedmodels_words_scr <- data4mixedmodels_words_scr[!is.na(data4mixedmodels_words_scr$subjectID),]

# make categorical variables factors
data4mixedmodels_words_scr$response_type <- factor(data4mixedmodels_words_scr$response_type, levels=c("SH","FH","FA"))
data4mixedmodels_words_scr$subjectID     <- factor(data4mixedmodels_words_scr$subjectID)
data4mixedmodels_words_scr$color_cross   <- factor(data4mixedmodels_words_scr$color_cross)

# I use sliding contrast (reasons for decision see Evernote entry 2019_08_06)
contrasts(data4mixedmodels_words_scr$response_type) <- contr.sdif(3)
contrasts(data4mixedmodels_words_scr$color_cross)   <- contr.sdif(2)

# add contrast as numerical covariate via model matrix (in order to enter contrasts individually in model and hence to use ||)
mm_c2    <- model.matrix( ~ response_type*color_cross, data4mixedmodels_words_scr) # stick in model matrix 6 columns

# attach to dataframe 
data4mixedmodels_words_scr[,(ncol(data4mixedmodels_words_scr)+1):(ncol(data4mixedmodels_words_scr)+3)] <- mm_c2[,c(4:6)]
names(data4mixedmodels_words_scr)[(ncol(data4mixedmodels_words_scr)-2):ncol(data4mixedmodels_words_scr)] <- c("red_minus_white", "FH_minus_SH:red_minus_white", "FA_minus_FH:red_minus_white")


# LMM only use trials that were preceded and followed by white cross (only 1277 trials are left; or only 1205 if incorrect word responses are also excluded)
  data4mixedmodels_words_scr$followed_or_preceded_by_red_cross <- FALSE
  
  for (i in 2:(nrow(data4mixedmodels_words_scr)-1)) {
    current_row <- data4mixedmodels_words_scr[i,]
    next_row <- data4mixedmodels_words_scr[(i+1),]
    previous_row <- data4mixedmodels_words_scr[(i-1),]
    if ((next_row$color_cross == 1  | previous_row$color_cross == 1)) {            
      data4mixedmodels_words_scr[i,]$followed_or_preceded_by_red_cross <- TRUE
    }     
  }

LMM_scr_cross_clean <- lmer(iscr_gng_resp_sqrt ~ FH_minus_SH + FA_minus_FH + red_minus_white + FH_minus_SH:red_minus_white + FA_minus_FH:red_minus_white + (1 + FH_minus_SH + red_minus_white + FH_minus_SH:red_minus_white  || subjectID), data=data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1 & data4mixedmodels_words_scr$followed_or_preceded_by_red_cross == FALSE,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

tab_model(LMM_scr_cross_clean,dv.labels = "SCR [square root and z transformed (µS)]", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)


# LMM use all trials
LMM_scr_cross <- lmer(iscr_gng_resp_sqrt ~ FH_minus_SH + FA_minus_FH + red_minus_white + FH_minus_SH:red_minus_white + FA_minus_FH:red_minus_white + (1 + FH_minus_SH + FA_minus_FH + red_minus_white + FH_minus_SH:red_minus_white + FA_minus_FH:red_minus_white  || subjectID), data=data4mixedmodels_words_scr[data4mixedmodels_words_scr$word_accuracy==1,], REML = TRUE, control=lmerControl(optimizer="bobyqa"))

tab_model(LMM_scr_cross,order.terms = c(1,6,4,3,5,2), dv.labels = "SCR [square root and z transformed (µS)]", pred.labels = pl,wrap.labels = 60, show.stat = TRUE, show.se = TRUE, string.se = "SE", show.loglik = TRUE, show.dev = TRUE, show.re.var = FALSE, show.ngroups = FALSE, show.icc = FALSE, show.r2 = TRUE)
```

<br>

#### Summary Control Analyses

The SCR seems not confounded by the valence of the following word, as there is no difference in SCR depending on word valence (see bar plor and LMM and ANOVA including the factor valence). Presentation of words of different valence after an error does not differentially affect the SCR. In addition, the number of positive and negative words following FAs, FHs, SHs is equal. Furthermore, it cannot be differentially confounded with prior responses, as only trials were included that were preceded and followed by correct responses to GNG stimuli and words. Thus, it may be possible to use SCR in my analyses, even though it does overlap with other events. 




<br>
<br>
<br>

# Summary

Even though the task design is not optimal to investigate the SCR, the control analyses suggest that valid inferences can be drawn from our analyses as the SCR is not confounded by previous or following events within the response window. Incorrect responses in the GNG task were associated with an increased SCR. There was no relation between SCR and priming effect. A nice post hoc finding is, that following incorrect responses, there is a positive relation between word categorization latency (a bit similar to PES) and SCR.


